{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sise/home/tommarz/hate_speech_detection'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# f = os.path.dirname(__file__)\n",
    "sys.path.append(os.path.join(os.getcwd(), \"../..\"))\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score, auc, roc_curve, \\\n",
    "    balanced_accuracy_score, precision_recall_curve, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from detection.detection_utils.factory import create_dir_if_missing\n",
    "from config.detection_config import user_level_execution_config, user_level_conf, post_level_execution_config\n",
    "\n",
    "sns.set(rc={'figure.figsize': (10, 10)}, font_scale=1.4)\n",
    "from scipy.optimize import minimize\n",
    "from utils.my_timeit import timeit\n",
    "from utils.general import init_log\n",
    "\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "sampler = optuna.samplers.TPESampler(**optuna.samplers.TPESampler.hyperopt_parameters())\n",
    "\n",
    "logger = init_log(\"user_level_simple_models\")\n",
    "\n",
    "def expect_f1(y_true, y_prob, thres):\n",
    "    idxs = np.where(y_prob >= thres)[0]\n",
    "    tp = y_prob[idxs].sum()\n",
    "    fp = len(idxs) - tp\n",
    "    idxs = np.where(y_prob < thres)[0]\n",
    "    fn = y_prob[idxs].sum()\n",
    "    return 2*tp / (2*tp + fp + fn)\n",
    "\n",
    "def optimal_threshold(y_true, y_prob):\n",
    "    y_prob = np.sort(y_prob)[::-1]\n",
    "    f1s = [expect_f1(y_true, y_prob, p) for p in y_prob]\n",
    "    thres = y_prob[np.argmax(f1s)]\n",
    "    return thres #, f1s \n",
    "\n",
    "def get_hs_count(current_preds, threshold=0.5):\n",
    "    return len(current_preds[current_preds >= threshold])\n",
    "\n",
    "os.chdir('/sise/home/tommarz/hate_speech_detection/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-13 00:39:22,830 - INFO     - user_level_simple_models - executing dataset gab...\u001b[0m\n",
      "Seed is: 3238357237\n"
     ]
    }
   ],
   "source": [
    "dataset = user_level_execution_config[\"inference_data\"]\n",
    "logger.info(f\"executing dataset {dataset}...\")\n",
    "model_name = post_level_execution_config[\"kwargs\"][\"model_name\"] # new_bert_fine_tuning\n",
    "user2pred = pd.read_parquet(f\"detection/outputs/{dataset}/{model_name}/user_level/split_by_posts/no_text/\")\n",
    "user2pred['user_id'] = user2pred['user_id'].astype(int)\n",
    "user2label_path = user_level_conf[dataset][\"data_path\"]\n",
    "sep = \",\"\n",
    "if user2label_path.endswith(\"tsv\"):\n",
    "    sep = \"\\t\"\n",
    "y = pd.read_csv(user2label_path, sep=sep, index_col=[0]).squeeze()\n",
    "# user2pred = user2pred[user2pred['user_id'].isin(labeled_users.index)]\n",
    "X = user2pred[user2pred['user_id'].isin(y.index)]\n",
    "seed = random.randrange(2 ** 32)\n",
    "# seed = 2334642105 #42 #338761188\n",
    "\n",
    "predictions_output_path = os.path.join(post_level_execution_config[\"evaluation\"][\"output_path\"], 'predictions.tsv')\n",
    "predictions_df = pd.read_csv(predictions_output_path, sep='\\t')\n",
    "y_true = predictions_df['y_true']\n",
    "y_prob = predictions_df['y_score']\n",
    "y_pred = predictions_df['y_pred']\n",
    "\n",
    "print(\"Seed is:\", seed)\n",
    "# fixed_threshold_num_of_posts(user2pred, labeled_users, output_path, dataset, test_ratio=0.2, random_state=seed)\n",
    "# relational_threshold(user2pred, labeled_users, output_path, dataset, test_ratio=0.2, random_state=seed)\n",
    "# dynamic_threshold_hs_score(user2pred, labeled_users, output_path, test_ratio=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fixed Threshold Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_threshold_method(X: pd.DataFrame, y: pd.Series, post_threshold=0.5, test_ratio=0.2, random_state=None, min_post_th=1, max_post_th=300):\n",
    "    \n",
    "    y_train, y_test = train_test_split(y, test_size=0.2, random_state=random_state, stratify=y)\n",
    "    X_train = X[X['user_id'].isin(y_train.index)]\n",
    "    X_test = X[X['user_id'].isin(y_test.index)]\n",
    "    print(f'Train Percent HS Users: {y_train.mean()}')\n",
    "    print(f'Test Percent HS Users: {y_test.mean()}')\n",
    "    \n",
    "    args = [post_threshold]\n",
    "    train_hs_count_df = X_train.groupby('user_id').predictions.agg(get_hs_count, *args)\n",
    "    min_num_of_posts_thresholds = range(max(min_post_th, train_hs_count_df.min()), min(max_post_th, train_hs_count_df.max())+1)\n",
    "    \n",
    "    train_preds = np.expand_dims(train_hs_count_df, axis=1) >= min_num_of_posts_thresholds\n",
    "    train_f1_scores = [f1_score(y_train, p) for p in train_preds.T]\n",
    "    best_f1_train, best_th = np.max(train_f1_scores), min_num_of_posts_thresholds[np.argmax(train_f1_scores)]\n",
    "    \n",
    "    test_hs_count_df = X_test.groupby('user_id').predictions.agg(get_hs_count, *args)\n",
    "    test_preds = test_hs_count_df >= best_th\n",
    "    test_f1_score = f1_score(y_test, test_preds)\n",
    "    \n",
    "    return best_th, best_f1_train, test_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Percent HS Users: 0.2475\n",
      "Test Percent HS Users: 0.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8, 0.4016309887869521, 0.388663967611336)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_threshold = optimal_threshold(y_true, y_prob)\n",
    "fixed_threshold_method(X, y, post_threshold=post_threshold, test_ratio=0.2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "def fixed_threshold_method_cv(X: pd.DataFrame, y: pd.Series,post_threshold=0.5, cv=5, test_ratio=0.2, random_state=None,min_post_th=1, max_post_th=300):\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=False, random_state=random_state)\n",
    "    y_train_val, y_test = train_test_split(y, test_size=0.2, random_state=random_state, stratify=y)\n",
    "    X_train_val = X[X['user_id'].isin(y_train_val.index)]\n",
    "    X_test = X[X['user_id'].isin(y_test.index)]\n",
    "\n",
    "    predictions_output_path = os.path.join(post_level_execution_config[\"evaluation\"][\"output_path\"], 'predictions.tsv')\n",
    "    predictions_df = pd.read_csv(predictions_output_path, sep='\\t')\n",
    "    y_true = predictions_df['y_true']\n",
    "    y_prob = predictions_df['y_score']\n",
    "    # y_pred = predictions_df['y_pred']\n",
    "    post_threshold = optimal_threshold(y_true, y_prob)\n",
    "\n",
    "    best_th_lst, train_f1_score_lst, val_f1_score_lst = np.array([]), np.array([]), np.array([])\n",
    "\n",
    "    for train_index, test_index in skf.split(y_train_val, y_train_val):\n",
    "        y_train, y_val = y_train_val.iloc[train_index], y_train_val.iloc[test_index]\n",
    "        X_train = X_train_val[X_train_val['user_id'].isin(y_train.index)]\n",
    "        X_val = X_train_val[X_train_val['user_id'].isin(y_val.index)]\n",
    "\n",
    "        args = [post_threshold]\n",
    "        train_hs_count_df = X_train.groupby('user_id').predictions.agg(get_hs_count, *args)\n",
    "        min_num_of_posts_thresholds = range(max(min_post_th, train_hs_count_df.min()), min(max_post_th, train_hs_count_df.max()))\n",
    "\n",
    "        train_preds = f(train_hs_count_df, min_num_of_posts_thresholds)\n",
    "        train_f1_scores = [f1_score(y_train, preds) for preds in train_preds]\n",
    "        train_f1_score = np.max(train_f1_scores)\n",
    "        best_th = min_num_of_posts_thresholds[np.argmax(train_f1_scores)]\n",
    "\n",
    "        val_hs_count_df = X_val.groupby('user_id').predictions.agg(get_hs_count, *args)\n",
    "        val_preds = get_user_preds(val_hs_count_df, best_th)\n",
    "        val_f1_score = f1_score(y_val, val_preds)\n",
    "\n",
    "        best_th_lst = np.append(best_th_lst, best_th)\n",
    "        train_f1_score_lst = np.append(train_f1_score_lst, train_f1_score)\n",
    "        val_f1_score_lst = np.append(val_f1_score_lst, val_f1_score)\n",
    "\n",
    "        print(best_th, train_f1_score, val_f1_score)\n",
    "    return  best_th_lst, train_f1_score_lst, val_f1_score_lst"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "predictions_output_path = os.path.join(post_level_execution_config[\"evaluation\"][\"output_path\"], 'predictions.tsv')\n",
    "predictions_df = pd.read_csv(predictions_output_path, sep='\\t')\n",
    "y_true = predictions_df['y_true']\n",
    "y_prob = predictions_df['y_score']\n",
    "# y_pred = predictions_df['y_pred']\n",
    "post_threshold = optimal_threshold(y_true, y_prob)\n",
    "best_th_lst, train_f1_score_lst, val_f1_score_lst = fixed_threshold_method_cv(user2pred, labeled_users, post_threshold)\n",
    "# best_th, train_f1, test_f1 \n",
    "np.median(best_th_lst).astype(int), np.mean(train_f1_score_lst), np.mean(val_f1_score_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Relational Threshold Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35105</td>\n",
       "      <td>0.269087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35105</td>\n",
       "      <td>0.031074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35105</td>\n",
       "      <td>0.769190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35105</td>\n",
       "      <td>0.026870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35105</td>\n",
       "      <td>0.068792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19840422</th>\n",
       "      <td>123832</td>\n",
       "      <td>0.189174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19840423</th>\n",
       "      <td>123832</td>\n",
       "      <td>0.299337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19840424</th>\n",
       "      <td>123832</td>\n",
       "      <td>0.504460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19840425</th>\n",
       "      <td>123832</td>\n",
       "      <td>0.231645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19840426</th>\n",
       "      <td>123832</td>\n",
       "      <td>0.174499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600178 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id  predictions\n",
       "0           35105     0.269087\n",
       "1           35105     0.031074\n",
       "2           35105     0.769190\n",
       "3           35105     0.026870\n",
       "4           35105     0.068792\n",
       "...           ...          ...\n",
       "19840422   123832     0.189174\n",
       "19840423   123832     0.299337\n",
       "19840424   123832     0.504460\n",
       "19840425   123832     0.231645\n",
       "19840426   123832     0.174499\n",
       "\n",
       "[600178 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>dest</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1175</td>\n",
       "      <td>4735</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1175</td>\n",
       "      <td>491</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1175</td>\n",
       "      <td>31</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1175</td>\n",
       "      <td>1</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1175</td>\n",
       "      <td>341</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68002</th>\n",
       "      <td>373817</td>\n",
       "      <td>8367</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68003</th>\n",
       "      <td>373817</td>\n",
       "      <td>19632</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68004</th>\n",
       "      <td>373817</td>\n",
       "      <td>276224</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68005</th>\n",
       "      <td>379860</td>\n",
       "      <td>36481</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68006</th>\n",
       "      <td>379406</td>\n",
       "      <td>75707</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68007 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       source    dest  weight\n",
       "0        1175    4735      90\n",
       "1        1175     491     236\n",
       "2        1175      31     117\n",
       "3        1175       1     118\n",
       "4        1175     341      79\n",
       "...       ...     ...     ...\n",
       "68002  373817    8367       5\n",
       "68003  373817   19632      12\n",
       "68004  373817  276224      13\n",
       "68005  379860   36481       8\n",
       "68006  379406   75707       5\n",
       "\n",
       "[68007 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_mentions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7fd49c3cf370>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user2pred.groupby('user_id').predictions.agg(get_hs_count, *args).rename('hs_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_hs_count = user2pred.query('`user_id` in @y.index').groupby('user_id').predictions.agg(get_hs_count, *args).rename('hs_count')\n",
    "mentions_hs_count = user2pred.query('`user_id` in @filtered_mentions_df.source').groupby('user_id').predictions.agg(get_hs_count, *args).rename('hs_count').reset_index()\n",
    "mentioned_by_hs_count =  user2pred.query('`user_id` in @filtered_mentions_df.dest').groupby('user_id').predictions.agg(get_hs_count, *args).rename('hs_count').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id_source</th>\n",
       "      <th>hs_count_source</th>\n",
       "      <th>user_id_dest</th>\n",
       "      <th>hs_count_dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1175</td>\n",
       "      <td>1366</td>\n",
       "      <td>4735</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1175</td>\n",
       "      <td>1366</td>\n",
       "      <td>491</td>\n",
       "      <td>3092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1175</td>\n",
       "      <td>1366</td>\n",
       "      <td>31</td>\n",
       "      <td>1319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1175</td>\n",
       "      <td>1366</td>\n",
       "      <td>1</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1175</td>\n",
       "      <td>1366</td>\n",
       "      <td>341</td>\n",
       "      <td>569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68002</th>\n",
       "      <td>373817</td>\n",
       "      <td>55</td>\n",
       "      <td>8367</td>\n",
       "      <td>3939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68003</th>\n",
       "      <td>373817</td>\n",
       "      <td>55</td>\n",
       "      <td>19632</td>\n",
       "      <td>1696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68004</th>\n",
       "      <td>373817</td>\n",
       "      <td>55</td>\n",
       "      <td>276224</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68005</th>\n",
       "      <td>379860</td>\n",
       "      <td>1</td>\n",
       "      <td>36481</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68006</th>\n",
       "      <td>379406</td>\n",
       "      <td>22</td>\n",
       "      <td>75707</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68007 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id_source  hs_count_source  user_id_dest  hs_count_dest\n",
       "0                1175             1366          4735            473\n",
       "1                1175             1366           491           3092\n",
       "2                1175             1366            31           1319\n",
       "3                1175             1366             1            188\n",
       "4                1175             1366           341            569\n",
       "...               ...              ...           ...            ...\n",
       "68002          373817               55          8367           3939\n",
       "68003          373817               55         19632           1696\n",
       "68004          373817               55        276224            224\n",
       "68005          379860                1         36481            161\n",
       "68006          379406               22         75707             10\n",
       "\n",
       "[68007 rows x 4 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_mentions_hs_count_df = pd.merge(\n",
    "    pd.merge(filtered_mentions_df, mentions_hs_count, left_on='source', right_on='user_id', how='left'),\n",
    "    mentioned_by_hs_count, left_on='dest', right_on='user_id', how='left', suffixes=('_source', '_dest')\n",
    ").fillna(0).astype(int).drop(columns=['source', 'dest', 'weight'])\n",
    "filtered_mentions_hs_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_dir = f\"hate_networks/outputs/{dataset.split('_')[0]}_networks/network_data/\"\n",
    "edges_dir = os.path.join(network_dir, \"edges\")\n",
    "mentions_df = pd.read_csv(os.path.join(edges_dir, \"data_users_mention_edges_df.tsv\"), sep='\\t')\n",
    "retweets_df = pd.read_csv(os.path.join(edges_dir, \"data_users_retweet_edges_df.tsv\"), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def relational_threshold_method(X: pd.DataFrame, y: pd.DataFrame, min_mention_threshold=3, test_ratio=0.2, post_threshold=0.5, random_state=None):\n",
    "    \"\"\"\n",
    "    Here we consider the assumption that relation to followers/followees effect the users' behaviour.\n",
    "    For each user - get his average HS score, and the average HS scores of his followers and followees.\n",
    "    then search for the optimal relational threshold to yield the best f1-score.\n",
    "    This threshold will be combined from a self-TH + followers-TH + followees-TH.\n",
    "\n",
    "    :param user2pred:\n",
    "    :param labeled_users:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    min_post_th, max_post_th = 1, 300\n",
    "\n",
    "    y_train, y_test = train_test_split(y, test_size=test_ratio, random_state=random_state, stratify=y)\n",
    "    train_df = X[X['user_id'].isin(y_train.index)]\n",
    "    test_df = X[X['user_id'].isin(y_test.index)]\n",
    "    # keep only mentions above the minimal threshold\n",
    "    filtered_mentions_df = mentions_df[mentions_df[\"weight\"] >= min_mention_threshold].reset_index(drop=True)\n",
    "    mentions_dict = {}  # users mentioned by the observed user\n",
    "    mentioned_by_dict = {}  # users mentioning the observed user\n",
    "    # mentions_df = mentions_df[(mentions_df['source'].isin(y.index)) | (mentions_df['dest'].isin(y.index))]\n",
    "    # retweets_df = retweets_df[(retweets_df['source'].isin(y.index)) | (retweets_df['dest'].isin(y.index))]\n",
    "    # mentions_dict = filtered_mentions_df.groupby('source')['dest'].apply(list) #.to_dict()\n",
    "    # mentioned_by_dict = filtered_mentions_df.groupby('dest')['source'].apply(list) #.to_dict()\n",
    "\n",
    "    args = [post_threshold]\n",
    "    user_hs_count = user2pred.query('`user_id` in @y.index').groupby('user_id').predictions.agg(get_hs_count, *args).rename('hs_count')\n",
    "    mentions_hs_count = user2pred.query('`user_id` in @filtered_mentions_df.source').groupby('user_id').predictions.agg(get_hs_count, *args).rename('hs_count').reset_index()\n",
    "    mentioned_by_hs_count =  user2pred.query('`user_id` in @filtered_mentions_df.dest').groupby('user_id').predictions.agg(get_hs_count, *args).rename('hs_count').reset_index()\n",
    "\n",
    "    filtered_mentions_hs_count_df = pd.merge(\n",
    "        pd.merge(filtered_mentions_df, mentions_hs_count, left_on='source', right_on='user_id', how='left'),\n",
    "        mentioned_by_hs_count, left_on='dest', right_on='user_id', how='left', suffixes=('_source', '_dest')\n",
    "    ).fillna(0).astype(int).drop(columns=['source', 'dest', 'weight'])\n",
    "    # filtered_mentions_hs_count_df\n",
    "\n",
    "    following_hs_df = filtered_mentions_hs_count_df.groupby('user_id_source').agg({'hs_count_dest': ['mean', 'count', 'median']})\n",
    "    following_hs_df.columns = [f'following_hs_{x[1]}' for x in following_hs_df.columns.to_flat_index()]\n",
    "    followers_hs_df = filtered_mentions_hs_count_df.groupby('user_id_dest').agg({'hs_count_source': ['mean', 'count', 'median']})\n",
    "    followers_hs_df.columns = [f'followers_hs_{x[1]}' for x in followers_hs_df.columns.to_flat_index()]\n",
    "\n",
    "    followees_mean_hs_count_df =  filtered_mentions_hs_count_df.groupby('user_id_source')['hs_count_dest'].mean().rename('following_mean_hs_count')\n",
    "    followers_mean_hs_count_df = filtered_mentions_hs_count_df.groupby('user_id_dest')['hs_count_source'].mean().rename('followers_mean_hs_count')\n",
    "\n",
    "    user_hs_count_followees_followers_mean_hs_count = pd.merge(\n",
    "        pd.merge(\n",
    "            user_hs_count.rename('hs_count'), following_hs_df, left_index=True, right_index=True, how='left'\n",
    "        ), followers_hs_df, left_index=True, right_index=True, how='left'\n",
    "    ).fillna(0)#.astype(int) #.sum(axis=1)\n",
    "    # user_hs_count_followees_followers_mean_hs_count\n",
    "\n",
    "    # user_hs_count_followees_followers_mean_hs_count.sum(axis=1).mean().astype(int)\n",
    "\n",
    "    cols = ['hs_count', 'following_hs_mean', 'followers_hs_mean']\n",
    "    X_train = user_hs_count_followees_followers_mean_hs_count.loc[y_train.index, cols]\n",
    "    X_test = user_hs_count_followees_followers_mean_hs_count.loc[y_test.index, cols]\n",
    "\n",
    "    def get_relational_model_preds(X, y, self_weight, followers_weight, following_weight, threshold):\n",
    "        preds = np.dot(X, [self_weight, followers_weight, following_weight]) >= threshold\n",
    "        return f1_score(y, preds)\n",
    "\n",
    "    def objective(trial, X, y):\n",
    "        self_weight = trial.suggest_float('self_weight', 0, 1)\n",
    "        # followers_weight = trial.suggest_discrete_uniform('followers_weight', 0, 1, 0.05)\n",
    "        followers_weight = trial.suggest_float('followers_weight', 0, 1)\n",
    "        following_weight = trial.suggest_float('following_weight', 0, 1)\n",
    "        # threshold = trial.suggest_float('threshold', 1, X_train.sum(axis=1).mean().astype(int))\n",
    "        threshold = trial.suggest_float('threshold', 1, 300)\n",
    "        return get_relational_model_preds(X, y, self_weight, followers_weight, following_weight, threshold)\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=sampler)  # Create a new study.\n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=1000, show_progress_bar=True)\n",
    "\n",
    "    test_f1 = get_relational_model_preds(X_test, y_test, **study.best_params)\n",
    "    best_f1 = study.best_value\n",
    "    print(study.best_params)\n",
    "    print( best_f1, test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:20<00:00, 49.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'self_weight': 0.9413367253455346, 'followers_weight': 0.5802410616285082, 'following_weight': 0.0018075924492777506, 'threshold': 10.743408265861822}\n",
      "0.44579780755176607 0.4307692307692308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "post_threshold = optimal_threshold(y_true, y_prob)\n",
    "relational_threshold_method(X, y, post_threshold=0.5, min_mention_threshold=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_threshold_method(X: pd.DataFrame, y: pd.DataFrame, test_ratio=0.2, random_state=None, proba_threshold=0.5):\n",
    "\n",
    "    hs_count_and_avg_score_per_user = X.groupby('user_id').agg(\n",
    "        avg_hs_score=(\"predictions\", \"mean\"),\n",
    "        hs_count=(\"predictions\", lambda p: get_hs_count(p, proba_threshold)))\n",
    "\n",
    "    y_train, y_test = train_test_split(y, test_size=test_ratio, random_state=random_state, stratify=y)\n",
    "    X_train = hs_count_and_avg_score_per_user.loc[y_train.index]\n",
    "    X_test = hs_count_and_avg_score_per_user.loc[y_test.index]\n",
    "    print(f'Train Percent HS Users: {y_train.mean()}')\n",
    "    print(f'Test Percent HS Users: {y_test.mean()}')\n",
    "\n",
    "    def calc_soft_threshold(arr, lower_bound, higher_bound, low_th, medium_th, high_th):\n",
    "        return arr[:, 1] >= np.where(arr[:, 0] < lower_bound, high_th, np.where(arr[:, 0] < higher_bound, medium_th, low_th))\n",
    "\n",
    "    def objective(trial):\n",
    "        lower_bound = trial.suggest_float(\"lower_bound\", 0.01, 0.15)\n",
    "        higher_bound = trial.suggest_float(\"higher_bound\", lower_bound + 0.01, 0.3)\n",
    "        low_th = trial.suggest_int(\"low_th\", 1, np.percentile(X_train['hs_count'].values, 20).astype(int)+1)\n",
    "        medium_th = trial.suggest_int(\"medium_th\", low_th+1, np.percentile(X_train['hs_count'].values, 40).astype(int)+1)\n",
    "        high_th = trial.suggest_int(\"high_th\", medium_th + 1, np.percentile(X_train['hs_count'].values, 60).astype(int)+1)\n",
    "\n",
    "        c0 = float(0.01 + lower_bound - higher_bound)\n",
    "        c1 = float(1 + medium_th - high_th)\n",
    "        c2 = float(1 + low_th - medium_th)\n",
    "\n",
    "        # Store the constraints as user attributes so that they can be restored after optimization.\n",
    "        trial.set_user_attr(\"constraint\", (c0, c1, c2))\n",
    "\n",
    "        y_pred = calc_soft_threshold(X_train.values, lower_bound, higher_bound, low_th, medium_th, high_th)\n",
    "\n",
    "        f1 = f1_score(y_train, y_pred)\n",
    "        return f1\n",
    "\n",
    "    def constraints(trial):\n",
    "        return trial.user_attrs[\"constraint\"]\n",
    "\n",
    "    sampler = optuna.samplers.NSGAIISampler(\n",
    "        constraints_func=constraints\n",
    "    )\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        sampler=sampler,\n",
    "    )\n",
    "    study.optimize(objective, n_trials=1000, show_progress_bar=True)\n",
    "    best_f1 = study.best_value\n",
    "    logger.info(f\"Max f1-score: {best_f1}\")\n",
    "\n",
    "    # y_true = test_g_df[\"label\"].values\n",
    "    y_pred = calc_soft_threshold(X_test.values, **study.best_params)\n",
    "    print(study.best_params)\n",
    "    print(best_f1, f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Percent HS Users: 0.2475\n",
      "Test Percent HS Users: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:05<00:00, 173.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-13 01:21:51,409 - INFO     - user_level_simple_models - Max f1-score: 0.5355371900826447\u001b[0m\n",
      "{'lower_bound': 0.1384709495203148, 'higher_bound': 0.20958431397356525, 'low_th': 3, 'medium_th': 12, 'high_th': 20}\n",
      "0.5355371900826447 0.5228758169934641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dynamic_threshold_method(X, y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "best_params = {'lower_bound': 0.14051533151937082, 'higher_bound': 0.23160535070882932, 'low_th': 7, 'medium_th': 12, 'high_th': 20}"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "eyal_hate_speech",
   "language": "python",
   "name": "eyal_hate_speech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
