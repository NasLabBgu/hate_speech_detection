{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sise/home/tommarz/hate_speech_detection'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# f = os.path.dirname(__file__)\n",
    "sys.path.append(os.path.join(os.getcwd(), \"../..\"))\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score, auc, roc_curve, \\\n",
    "    balanced_accuracy_score, precision_recall_curve, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from detection.detection_utils.factory import create_dir_if_missing\n",
    "from config.detection_config import user_level_execution_config, user_level_conf, post_level_execution_config\n",
    "\n",
    "sns.set(rc={'figure.figsize': (10, 10)}, font_scale=1.4)\n",
    "from scipy.optimize import minimize\n",
    "from utils.my_timeit import timeit\n",
    "from utils.general import init_log\n",
    "\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "sampler = optuna.samplers.TPESampler(**optuna.samplers.TPESampler.hyperopt_parameters())\n",
    "\n",
    "logger = init_log(\"user_level_simple_models\")\n",
    "\n",
    "def expect_f1(y_true, y_prob, thres):\n",
    "    idxs = np.where(y_prob >= thres)[0]\n",
    "    tp = y_prob[idxs].sum()\n",
    "    fp = len(idxs) - tp\n",
    "    idxs = np.where(y_prob < thres)[0]\n",
    "    fn = y_prob[idxs].sum()\n",
    "    return 2*tp / (2*tp + fp + fn)\n",
    "\n",
    "def optimal_threshold(y_true, y_prob):\n",
    "    y_prob = np.sort(y_prob)[::-1]\n",
    "    f1s = [expect_f1(y_true, y_prob, p) for p in y_prob]\n",
    "    thres = y_prob[np.argmax(f1s)]\n",
    "    return thres #, f1s \n",
    "\n",
    "def get_hs_count(current_preds, threshold=0.5):\n",
    "    return len(current_preds[current_preds >= threshold])\n",
    "\n",
    "os.chdir('/sise/home/tommarz/hate_speech_detection/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-13 00:39:22,830 - INFO     - user_level_simple_models - executing dataset gab...\u001b[0m\n",
      "Seed is: 3238357237\n"
     ]
    }
   ],
   "source": [
    "dataset = user_level_execution_config[\"inference_data\"]\n",
    "logger.info(f\"executing dataset {dataset}...\")\n",
    "model_name = post_level_execution_config[\"kwargs\"][\"model_name\"] # new_bert_fine_tuning\n",
    "user2pred = pd.read_parquet(f\"detection/outputs/{dataset}/{model_name}/user_level/split_by_posts/no_text/\")\n",
    "user2pred['user_id'] = user2pred['user_id'].astype(int)\n",
    "user2label_path = user_level_conf[dataset][\"data_path\"]\n",
    "sep = \",\"\n",
    "if user2label_path.endswith(\"tsv\"):\n",
    "    sep = \"\\t\"\n",
    "y = pd.read_csv(user2label_path, sep=sep, index_col=[0]).squeeze()\n",
    "# user2pred = user2pred[user2pred['user_id'].isin(labeled_users.index)]\n",
    "X = user2pred[user2pred['user_id'].isin(y.index)]\n",
    "seed = random.randrange(2 ** 32)\n",
    "# seed = 2334642105 #42 #338761188\n",
    "\n",
    "predictions_output_path = os.path.join(post_level_execution_config[\"evaluation\"][\"output_path\"], 'predictions.tsv')\n",
    "predictions_df = pd.read_csv(predictions_output_path, sep='\\t')\n",
    "y_true = predictions_df['y_true']\n",
    "y_prob = predictions_df['y_score']\n",
    "y_pred = predictions_df['y_pred']\n",
    "\n",
    "print(\"Seed is:\", seed)\n",
    "# fixed_threshold_num_of_posts(user2pred, labeled_users, output_path, dataset, test_ratio=0.2, random_state=seed)\n",
    "# relational_threshold(user2pred, labeled_users, output_path, dataset, test_ratio=0.2, random_state=seed)\n",
    "# dynamic_threshold_hs_score(user2pred, labeled_users, output_path, test_ratio=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fixed Threshold Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_threshold_method(X: pd.DataFrame, y: pd.Series, post_threshold=0.5, test_ratio=0.2, random_state=None, min_post_th=1, max_post_th=300):\n",
    "    \n",
    "    y_train, y_test = train_test_split(y, test_size=0.2, random_state=random_state, stratify=y)\n",
    "    X_train = X[X['user_id'].isin(y_train.index)]\n",
    "    X_test = X[X['user_id'].isin(y_test.index)]\n",
    "    print(f'Train Percent HS Users: {y_train.mean()}')\n",
    "    print(f'Test Percent HS Users: {y_test.mean()}')\n",
    "    \n",
    "    args = [post_threshold]\n",
    "    train_hs_count_df = X_train.groupby('user_id').predictions.agg(get_hs_count, *args)\n",
    "    min_num_of_posts_thresholds = range(max(min_post_th, train_hs_count_df.min()), min(max_post_th, train_hs_count_df.max())+1)\n",
    "    \n",
    "    train_preds = np.expand_dims(train_hs_count_df, axis=1) >= min_num_of_posts_thresholds\n",
    "    train_f1_scores = [f1_score(y_train, p) for p in train_preds.T]\n",
    "    best_f1_train, best_th = np.max(train_f1_scores), min_num_of_posts_thresholds[np.argmax(train_f1_scores)]\n",
    "    \n",
    "    test_hs_count_df = X_test.groupby('user_id').predictions.agg(get_hs_count, *args)\n",
    "    test_preds = test_hs_count_df >= best_th\n",
    "    test_f1_score = f1_score(y_test, test_preds)\n",
    "    \n",
    "    return best_th, best_f1_train, test_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Percent HS Users: 0.2475\n",
      "Test Percent HS Users: 0.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8, 0.4016309887869521, 0.388663967611336)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_threshold = optimal_threshold(y_true, y_prob)\n",
    "fixed_threshold_method(X, y, post_threshold=post_threshold, test_ratio=0.2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "def fixed_threshold_method_cv(X: pd.DataFrame, y: pd.Series,post_threshold=0.5, cv=5, test_ratio=0.2, random_state=None,min_post_th=1, max_post_th=300):\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=False, random_state=random_state)\n",
    "    y_train_val, y_test = train_test_split(y, test_size=0.2, random_state=random_state, stratify=y)\n",
    "    X_train_val = X[X['user_id'].isin(y_train_val.index)]\n",
    "    X_test = X[X['user_id'].isin(y_test.index)]\n",
    "\n",
    "    predictions_output_path = os.path.join(post_level_execution_config[\"evaluation\"][\"output_path\"], 'predictions.tsv')\n",
    "    predictions_df = pd.read_csv(predictions_output_path, sep='\\t')\n",
    "    y_true = predictions_df['y_true']\n",
    "    y_prob = predictions_df['y_score']\n",
    "    # y_pred = predictions_df['y_pred']\n",
    "    post_threshold = optimal_threshold(y_true, y_prob)\n",
    "\n",
    "    best_th_lst, train_f1_score_lst, val_f1_score_lst = np.array([]), np.array([]), np.array([])\n",
    "\n",
    "    for train_index, test_index in skf.split(y_train_val, y_train_val):\n",
    "        y_train, y_val = y_train_val.iloc[train_index], y_train_val.iloc[test_index]\n",
    "        X_train = X_train_val[X_train_val['user_id'].isin(y_train.index)]\n",
    "        X_val = X_train_val[X_train_val['user_id'].isin(y_val.index)]\n",
    "\n",
    "        args = [post_threshold]\n",
    "        train_hs_count_df = X_train.groupby('user_id').predictions.agg(get_hs_count, *args)\n",
    "        min_num_of_posts_thresholds = range(max(min_post_th, train_hs_count_df.min()), min(max_post_th, train_hs_count_df.max()))\n",
    "\n",
    "        train_preds = f(train_hs_count_df, min_num_of_posts_thresholds)\n",
    "        train_f1_scores = [f1_score(y_train, preds) for preds in train_preds]\n",
    "        train_f1_score = np.max(train_f1_scores)\n",
    "        best_th = min_num_of_posts_thresholds[np.argmax(train_f1_scores)]\n",
    "\n",
    "        val_hs_count_df = X_val.groupby('user_id').predictions.agg(get_hs_count, *args)\n",
    "        val_preds = get_user_preds(val_hs_count_df, best_th)\n",
    "        val_f1_score = f1_score(y_val, val_preds)\n",
    "\n",
    "        best_th_lst = np.append(best_th_lst, best_th)\n",
    "        train_f1_score_lst = np.append(train_f1_score_lst, train_f1_score)\n",
    "        val_f1_score_lst = np.append(val_f1_score_lst, val_f1_score)\n",
    "\n",
    "        print(best_th, train_f1_score, val_f1_score)\n",
    "    return  best_th_lst, train_f1_score_lst, val_f1_score_lst"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "predictions_output_path = os.path.join(post_level_execution_config[\"evaluation\"][\"output_path\"], 'predictions.tsv')\n",
    "predictions_df = pd.read_csv(predictions_output_path, sep='\\t')\n",
    "y_true = predictions_df['y_true']\n",
    "y_prob = predictions_df['y_score']\n",
    "# y_pred = predictions_df['y_pred']\n",
    "post_threshold = optimal_threshold(y_true, y_prob)\n",
    "best_th_lst, train_f1_score_lst, val_f1_score_lst = fixed_threshold_method_cv(user2pred, labeled_users, post_threshold)\n",
    "# best_th, train_f1, test_f1 \n",
    "np.median(best_th_lst).astype(int), np.mean(train_f1_score_lst), np.mean(val_f1_score_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Relational Threshold Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35105</td>\n",
       "      <td>0.269087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35105</td>\n",
       "      <td>0.031074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35105</td>\n",
       "      <td>0.769190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35105</td>\n",
       "      <td>0.026870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35105</td>\n",
       "      <td>0.068792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19840422</th>\n",
       "      <td>123832</td>\n",
       "      <td>0.189174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19840423</th>\n",
       "      <td>123832</td>\n",
       "      <td>0.299337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19840424</th>\n",
       "      <td>123832</td>\n",
       "      <td>0.504460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19840425</th>\n",
       "      <td>123832</td>\n",
       "      <td>0.231645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19840426</th>\n",
       "      <td>123832</td>\n",
       "      <td>0.174499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600178 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id  predictions\n",
       "0           35105     0.269087\n",
       "1           35105     0.031074\n",
       "2           35105     0.769190\n",
       "3           35105     0.026870\n",
       "4           35105     0.068792\n",
       "...           ...          ...\n",
       "19840422   123832     0.189174\n",
       "19840423   123832     0.299337\n",
       "19840424   123832     0.504460\n",
       "19840425   123832     0.231645\n",
       "19840426   123832     0.174499\n",
       "\n",
       "[600178 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mentions_hs_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 39\u001b[0m\n\u001b[1;32m     35\u001b[0m mentions_dict \u001b[38;5;241m=\u001b[39m filtered_mentions_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdest\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mlist\u001b[39m) \u001b[38;5;66;03m#.to_dict()\u001b[39;00m\n\u001b[1;32m     36\u001b[0m mentioned_by_dict \u001b[38;5;241m=\u001b[39m filtered_mentions_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdest\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mlist\u001b[39m) \u001b[38;5;66;03m#.to_dict()\u001b[39;00m\n\u001b[1;32m     38\u001b[0m filtered_mentions_hs_count_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(\n\u001b[0;32m---> 39\u001b[0m     pd\u001b[38;5;241m.\u001b[39mmerge(filtered_mentions_df, \u001b[43mmentions_hs_count\u001b[49m, left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m, right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     40\u001b[0m     mentioned_by_hs_count, left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdest\u001b[39m\u001b[38;5;124m'\u001b[39m, right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, suffixes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_source\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_dest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     41\u001b[0m )\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdest\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# filtered_mentions_hs_count_df\u001b[39;00m\n\u001b[1;32m     44\u001b[0m following_hs_df \u001b[38;5;241m=\u001b[39m filtered_mentions_hs_count_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id_source\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39magg({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhs_count_dest\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mentions_hs_count' is not defined"
     ]
    }
   ],
   "source": [
    "# def relational_threshold_method(X: pd.DataFrame, y: pd.DataFrame, min_mention_threshold=3, test_ratio= 0.2, post_threshold=0.5, random_state=None):\n",
    "\"\"\"\n",
    "Here we consider the assumption that relation to followers/followees effect the users' behaviour.\n",
    "For each user - get his average HS score, and the average HS scores of his followers and followees.\n",
    "then search for the optimal relational threshold to yield the best f1-score.\n",
    "This threshold will be combined from a self-TH + followers-TH + followees-TH.\n",
    "\n",
    ":param user2pred:\n",
    ":param labeled_users:\n",
    ":return:\n",
    "\"\"\"\n",
    "\n",
    "min_post_th, max_post_th = 1, 300\n",
    "random_state = 42\n",
    "min_mention_threshold = 5\n",
    "y_train, y_test = train_test_split(y, test_size=0.2, random_state=random_state, stratify=y)\n",
    "train_df = X[X['user_id'].isin(y_train.index)]\n",
    "test_df = X[X['user_id'].isin(y_test.index)]\n",
    "\n",
    "post_threshold = optimal_threshold(y_true, y_prob)\n",
    "args = [post_threshold]\n",
    "train_hs_count_df = train_df.groupby('user_id').predictions.agg(get_hs_count, *args)\n",
    "min_num_of_posts_thresholds = range(max(min_post_th, train_hs_count_df.min()), min(max_post_th, train_hs_count_df.max())+1)\n",
    "\n",
    "network_dir = f\"hate_networks/outputs/{dataset.split('_')[0]}_networks/network_data/\"\n",
    "edges_dir = os.path.join(network_dir, \"edges\")\n",
    "mentions_df = pd.read_csv(os.path.join(edges_dir, \"data_users_mention_edges_df.tsv\"), sep='\\t')\n",
    "retweets_df = pd.read_csv(os.path.join(edges_dir, \"data_users_retweet_edges_df.tsv\"), sep='\\t')\n",
    "# keep only mentions above the minimal threshold\n",
    "filtered_mentions_df = mentions_df[mentions_df[\"weight\"] >= min_mention_threshold].reset_index(drop=True)\n",
    "mentions_dict = {}  # users mentioned by the observed user\n",
    "mentioned_by_dict = {}  # users mentioning the observed user\n",
    "# mentions_df = mentions_df[(mentions_df['source'].isin(y.index)) | (mentions_df['dest'].isin(y.index))]\n",
    "# retweets_df = retweets_df[(retweets_df['source'].isin(y.index)) | (retweets_df['dest'].isin(y.index))]\n",
    "# mentions_dict = filtered_mentions_df.groupby('source')['dest'].apply(list) #.to_dict()\n",
    "# mentioned_by_dict = filtered_mentions_df.groupby('dest')['source'].apply(list) #.to_dict()\n",
    "\n",
    "filtered_mentions_hs_count_df = pd.merge(\n",
    "    pd.merge(filtered_mentions_df, mentions_hs_count, left_on='source', right_on='user_id', how='left'),\n",
    "    mentioned_by_hs_count, left_on='dest', right_on='user_id', how='left', suffixes=('_source', '_dest')\n",
    ").fillna(0).astype(int).drop(columns=['source', 'dest', 'weight'])\n",
    "# filtered_mentions_hs_count_df\n",
    "\n",
    "following_hs_df = filtered_mentions_hs_count_df.groupby('user_id_source').agg({'hs_count_dest': ['mean', 'count', 'median']})\n",
    "following_hs_df.columns = [f'following_hs_{x[1]}' for x in following_hs_df.columns.to_flat_index()]\n",
    "followers_hs_df = filtered_mentions_hs_count_df.groupby('user_id_dest').agg({'hs_count_source': ['mean', 'count', 'median']})\n",
    "followers_hs_df.columns = [f'followers_hs_{x[1]}' for x in followers_hs_df.columns.to_flat_index()]\n",
    "\n",
    "followees_mean_hs_count_df =  filtered_mentions_hs_count_df.groupby('user_id_source')['hs_count_dest'].mean().rename('following_mean_hs_count')\n",
    "followers_mean_hs_count_df = filtered_mentions_hs_count_df.groupby('user_id_dest')['hs_count_source'].mean().rename('followers_mean_hs_count')\n",
    "\n",
    "user_hs_count_followees_followers_mean_hs_count = pd.merge(\n",
    "    pd.merge(\n",
    "        user_hs_count.rename('hs_count'), following_hs_df, left_index=True, right_index=True, how='left'\n",
    "    ), followers_hs_df, left_index=True, right_index=True, how='left'\n",
    ").fillna(0)#.astype(int) #.sum(axis=1)\n",
    "# user_hs_count_followees_followers_mean_hs_count\n",
    "\n",
    "# user_hs_count_followees_followers_mean_hs_count.sum(axis=1).mean().astype(int)\n",
    "\n",
    "cols = ['hs_count', 'following_hs_mean', 'followers_hs_mean']\n",
    "X_train = user_hs_count_followees_followers_mean_hs_count.loc[y_train.index, cols]\n",
    "X_test = user_hs_count_followees_followers_mean_hs_count.loc[y_test.index, cols]\n",
    "\n",
    "def get_relational_model_preds(X, y, self_weight, followers_weight, following_weight, threshold):\n",
    "    preds = np.dot(X, [self_weight, followers_weight, following_weight]) >= threshold\n",
    "    return f1_score(y, preds)\n",
    "\n",
    "def objective(trial, X, y):\n",
    "    self_weight = trial.suggest_float('self_weight', 0, 1)\n",
    "    # followers_weight = trial.suggest_discrete_uniform('followers_weight', 0, 1, 0.05)\n",
    "    followers_weight = trial.suggest_float('followers_weight', 0, 1)\n",
    "    following_weight = trial.suggest_float('following_weight', 0, 1)\n",
    "    threshold = trial.suggest_float('threshold', 1, X.sum(axis=1).mean().astype(int))\n",
    "    return get_relational_model_preds(X, y, self_weight, followers_weight, following_weight, threshold)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=sampler)  # Create a new study.\n",
    "study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=100, show_progress_bar=True)\n",
    "\n",
    "test_f1 = get_relational_model_preds(X_test, y_test, **study.best_params)\n",
    "print(study.best_params, study.best_value)\n",
    "print( best_f1, test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_threshold = optimal_threshold(y_true, y_prob)\n",
    "relational_threshold_method(X, y, post_threshold=post_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2671258613467215"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(X_train['avg_hs_score'].values, 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_threshold_method(X: pd.DataFrame, y: pd.DataFrame, test_ratio=0.2, random_state=None, proba_threshold=0.5):\n",
    "\n",
    "    hs_count_and_avg_score_per_user = X.groupby('user_id').agg(\n",
    "        avg_hs_score=(\"predictions\", \"mean\"),\n",
    "        hs_count=(\"predictions\", lambda p: get_hs_count(p, proba_threshold)))\n",
    "\n",
    "    y_train, y_test = train_test_split(y, test_size=test_ratio, random_state=random_state, stratify=y)\n",
    "    X_train = hs_count_and_avg_score_per_user.loc[y_train.index]\n",
    "    X_test = hs_count_and_avg_score_per_user.loc[y_test.index]\n",
    "    print(f'Train Percent HS Users: {y_train.mean()}')\n",
    "    print(f'Test Percent HS Users: {y_test.mean()}')\n",
    "\n",
    "    def calc_soft_threshold(arr, lower_bound, higher_bound, low_th, medium_th, high_th):\n",
    "        return arr[:, 1] >= np.where(arr[:, 0] < lower_bound, high_th, np.where(arr[:, 0] < higher_bound, medium_th, low_th))\n",
    "\n",
    "    def objective(trial):\n",
    "        lower_bound = trial.suggest_float(\"lower_bound\", 0.01, 0.15)\n",
    "        higher_bound = trial.suggest_float(\"higher_bound\", lower_bound + 0.01, 0.3)\n",
    "        low_th = trial.suggest_int(\"low_th\", 1, np.percentile(X_train['hs_count'].values, 20).astype(int)+1)\n",
    "        medium_th = trial.suggest_int(\"medium_th\", low_th+1, np.percentile(X_train['hs_count'].values, 40).astype(int)+1)\n",
    "        high_th = trial.suggest_int(\"high_th\", medium_th + 1, np.percentile(X_train['hs_count'].values, 60).astype(int)+1)\n",
    "\n",
    "        c0 = float(0.01 + lower_bound - higher_bound)\n",
    "        c1 = float(1 + medium_th - high_th)\n",
    "        c2 = float(1 + low_th - medium_th)\n",
    "\n",
    "        # Store the constraints as user attributes so that they can be restored after optimization.\n",
    "        trial.set_user_attr(\"constraint\", (c0, c1, c2))\n",
    "\n",
    "        y_pred = calc_soft_threshold(X_train.values, lower_bound, higher_bound, low_th, medium_th, high_th)\n",
    "\n",
    "        f1 = f1_score(y_train, y_pred)\n",
    "        return f1\n",
    "\n",
    "    def constraints(trial):\n",
    "        return trial.user_attrs[\"constraint\"]\n",
    "\n",
    "    sampler = optuna.samplers.NSGAIISampler(\n",
    "        constraints_func=constraints\n",
    "    )\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        sampler=sampler,\n",
    "    )\n",
    "    study.optimize(objective, n_trials=5000, show_progress_bar=True)\n",
    "    best_f1 = study.best_value\n",
    "    logger.info(f\"Max f1-score: {best_f1}\")\n",
    "\n",
    "    # y_true = test_g_df[\"label\"].values\n",
    "    y_pred = calc_soft_threshold(X_test.values, lower_bound, higher_bound, low_th, medium_th, high_th)\n",
    "    print(study.best_params)\n",
    "    print(f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Percent HS Users: 0.2475\n",
      "Test Percent HS Users: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:42<00:00, 118.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-13 00:34:55,536 - INFO     - user_level_simple_models - Max f1-score: 0.5344262295081967\u001b[0m\n",
      "{'lower_bound': 0.1385712536942841, 'higher_bound': 0.28751813627617046, 'low_th': 6, 'medium_th': 12, 'high_th': 20}\n",
      "0.5342465753424658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dynamic_threshold_method(X, y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "best_params = {'lower_bound': 0.14051533151937082, 'higher_bound': 0.23160535070882932, 'low_th': 7, 'medium_th': 12, 'high_th': 20}"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "eyal_hate_speech",
   "language": "python",
   "name": "eyal_hate_speech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
