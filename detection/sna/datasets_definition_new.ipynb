{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemTotal:       263906928 kB\n",
      "MemFree:        131092968 kB\n",
      "MemAvailable:   242471312 kB\n"
     ]
    }
   ],
   "source": [
    "!cat /proc/meminfo | grep Mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import dill\n",
    "import warnings\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sise/home/tommarz/hate_speech_detection'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_dir = '/sise/home/tommarz/hate_speech_detection/'\n",
    "detection_dir = os.path.join(main_dir, 'detection')\n",
    "# experiments_dir = os.path.join(detection_dir, 'experiments')\n",
    "sna_dir = os.path.join(detection_dir, 'sna')\n",
    "os.chdir(main_dir)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.data_config import path_confs\n",
    "from config.detection_config import user_level_execution_config, user_level_conf, post_level_execution_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "graphs_dict_path = os.path.join(sna_dir, 'graphs_dict.pkl')\n",
    "graphs_dict_path"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "reposts_dict = {}\n",
    "labeled_nodes_dict = {}\n",
    "# doc_vectors_dict = {}\n",
    "# graphs_with_docs_dict = {}\n",
    "# label_count_dict = {}\n",
    "# label_percent_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_vectors(dataset):\n",
    "    # if dataset in doc_vectors_dict:\n",
    "    #     return doc_vectors_dict[dataset]\n",
    "    doc_vectors = pickle.load(open(path_confs[dataset]['doc_vectors'], \"rb\"))\n",
    "    # doc_vectors_dict[dataset] = doc_vectors\n",
    "    return doc_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_labels(dataset):\n",
    "    if dataset in labeled_nodes_dict:\n",
    "        return labeled_nodes_dict[dataset]\n",
    "    user2label_path = user_level_conf[dataset][\"data_path\"]\n",
    "    sep = \",\"\n",
    "    if user2label_path.endswith(\"tsv\"):\n",
    "        sep = \"\\t\"\n",
    "    y = pd.read_csv(user2label_path, sep=sep, index_col=[0]).squeeze()\n",
    "    y.index = y.index.astype('str')\n",
    "    labeled_nodes_dict[dataset] = y\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reposts_graph(dataset, min_weight=1, graphs_dict = {}):\n",
    "    if dataset in graphs_dict:\n",
    "        g = graphs_dict[dataset]\n",
    "        filtered_edges = g.es.select(weight_ge=min_weight)\n",
    "        return g.subgraph_edges(filtered_edges)\n",
    "    reposts_path = path_confs[dataset]['reposts']\n",
    "    if reposts_path.endswith('.txt'):\n",
    "        reposts_df = pd.read_csv(reposts_path, sep='\\t', header=None, names=['source', 'target', 'weight'])\n",
    "    elif reposts_path.endswith('.tsv'):\n",
    "        if dataset == 'truth':\n",
    "            pd.read_csv(reposts_path, sep='\\t', names=['source', 'target', 'retruths_list', 'weight'], skiprows=1)\n",
    "        else:\n",
    "            reposts_df = pd.read_csv(reposts_path, sep='\\t', names=['source', 'target', 'weight'], skiprows=1)\n",
    "    elif reposts_path.endswith('.csv'):\n",
    "        reposts_df = pd.read_csv(reposts_path, header=None, names=['source', 'target', 'weight'])\n",
    "    else:\n",
    "        reposts_edge_dict = pickle.load(open(reposts_path, \"rb\"))\n",
    "        reposts_edge_list = [[k[0], k[1], v] for k,v in tqdm(reposts_edge_dict.items())]\n",
    "        reposts_df = pd.DataFrame(reposts_edge_list, columns=['source', 'target', 'weight'])\n",
    "    reposts_df['source'] = reposts_df['source'].astype(str)\n",
    "    reposts_df['target'] = reposts_df['target'].astype(str)\n",
    "    reposts_dict[dataset] = reposts_df\n",
    "    edges = [tuple(x) for x in reposts_df[['source', 'target', 'weight']].values]\n",
    "    g = ig.Graph.TupleList(edges, edge_attrs=['weight'], directed=True)    \n",
    "    y = get_user_labels(dataset)\n",
    "    g.vs['label'] = [y.loc[e['name']] if e['name'] in y.index else -1 for e in g.vs]\n",
    "    # g.vs.select(_degree=0).delete()\n",
    "    g.simplify(multiple=True, loops=True, combine_edges='sum')\n",
    "    g['name'] = dataset\n",
    "    graphs_dict[dataset] = g\n",
    "    filtered_edges = g.es.select(weight_ge=min_weight)\n",
    "    return g.subgraph_edges(filtered_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_largest_weak_cc(g):\n",
    "    # Assuming 'g' is your igraph Graph\n",
    "    components = g.components(mode='WEAK')  # Find weakly connected components\n",
    "    return components.giant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labeled_nodes(g):\n",
    "    return g.vs.select(lambda v: v['label']!=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results_latex(df, name):\n",
    "    with open(os.path.join(method_output_path, f'{method_name}_{name}_results_latex.txt'), 'w') as f:\n",
    "        res = ' & '.join([f'${m:.3f} \\pm {s:.3f}$' for m, s in df.values.reshape(-1, 2)]) + '\\\\\\\\'\n",
    "        f.write(res)\n",
    "\n",
    "def get_best_results_from_gs(gs):\n",
    "    train_results = pd.DataFrame(pd.DataFrame.from_dict(gs.cv_results_).loc[\n",
    "                                     gs.best_index_, [c for c in gs.cv_results_ if\n",
    "                                                      'mean_train' in c or 'std_train' in c]].values.reshape(-1, 2),\n",
    "                                 columns=['mean', 'std'], index=scoring_names)\n",
    "    test_results = pd.DataFrame(pd.DataFrame.from_dict(gs.cv_results_).loc[gs.best_index_, [c for c in gs.cv_results_ if\n",
    "                                                                                            'mean_test' in c or 'std_test' in c]].values.reshape(\n",
    "        -1, 2), columns=['mean', 'std'], index=scoring_names)\n",
    "    # best_hyper_params = pd.Series(gs.best_estimator_.get_params()).rename('value').to_csv(os.path.join(method_output_path, 'best_hyperparams.csv'))\n",
    "    return {'train': train_results, 'test': test_results}\n",
    "\n",
    "def get_best_results_from_cv_results(cv_results):\n",
    "    train_results =  pd.DataFrame(pd.DataFrame.from_dict(cv_results).loc[:, [c for c in cv_results if  'mean_train' in c or 'std_train' in c]].values.reshape(-1,2), columns=['mean', 'std'], index=scoring_names)\n",
    "    test_results =  pd.DataFrame(pd.DataFrame.from_dict(cv_results).loc[:, [c for c in cv_results if  'mean_test' in c or 'std_test' in c]].values.reshape(-1,2), columns=['mean', 'std'], index=scoring_names)\n",
    "    return {'train': train_results, 'test': test_results}\n",
    "\n",
    "def write_best_results_and_params_from_gs(gs):\n",
    "    results_dict = get_best_results_from_gs(gs)\n",
    "    pd.Series(gs.best_estimator_.get_params()).rename('value').to_csv(os.path.join(method_output_path, 'best_hyperparams.csv'))\n",
    "    pd.Series(gs.best_estimator_.get_learned_params()).rename('value').to_csv(os.path.join(method_output_path, 'best_params.csv'))\n",
    "\n",
    "    for name, df in results_dict.items():\n",
    "        write_results_latex(df, name=name)\n",
    "        df.to_csv(os.path.join(method_output_path, f'best_results_{name}.csv'))\n",
    "        \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_graph_with_docs(dataset):\n",
    "    if dataset in graphs_with_docs_dict:\n",
    "        return graphs_with_docs_dict[dataset]\n",
    "    G = graphs_dict.get(dataset, get_reposts_graph(dataset))\n",
    "    doc_vectors = doc_vectors_dict.get(dataset, get_doc_vectors(dataset))\n",
    "    H = G.subgraph(doc_vectors).copy()\n",
    "    graphs_with_docs_dict[dataset] = H\n",
    "    return H"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_label_count_and_percent(dataset):\n",
    "    G = get_graph_with_docs(dataset)\n",
    "    y = get_user_labels(dataset)\n",
    "    ngbrs_labels = {n : np.array([y[ngbr] if ngbr in y.index else 2 for ngbr in G.neighbors(n)]) for n in tqdm(G.nodes())}\n",
    "    ngbrs_labels_count = {k: Counter(v) for k,v in ngbrs_labels.items()}\n",
    "    label_count_df = pd.DataFrame.from_dict(ngbrs_labels_count, orient='index').fillna(0).astype(int).sort_index(axis=0).sort_index(axis=1)\n",
    "    return label_count_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def power_law(x, alpha, C):\n",
    "    return C * x**(-alpha)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_labeled_nodes(dataset: str = '', g: ig.Graph = None):\n",
    "    if dataset == '' and g is not None:\n",
    "        dataset = g['name']\n",
    "    elif g is None and dataset!='':\n",
    "        g = graphs_dict[dataset]\n",
    "    else:\n",
    "        raise ValueError(\"Both @dataset and @g can't be empty\")\n",
    "    y = get_user_labels(dataset)\n",
    "    labeled_nodes = g.vs.select(lambda v:  v['label'] != -1)\n",
    "    return labeled_nodes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Replace 'degrees' with your actual data containing node degrees\n",
    "def calc_power_law_exp(degrees):\n",
    "\n",
    "    # Convert the degrees to numpy arrays\n",
    "    x = np.array(degrees)\n",
    "\n",
    "    # Count the frequency of each degree\n",
    "    degree_counts = np.bincount(x)\n",
    "\n",
    "    # Remove the zero-degree entries\n",
    "    x = np.nonzero(degree_counts)[0]\n",
    "    y = degree_counts[x]\n",
    "\n",
    "    # Perform the curve fitting\n",
    "    popt, _ = curve_fit(power_law, x, y)\n",
    "    alpha, C = popt\n",
    "    \n",
    "    plt.scatter(x, y, label=\"Data\")\n",
    "    plt.plot(x, power_law(x, alpha, C), color='red', label=f\"Power-law fit (alpha={alpha:.2f})\")\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel(\"Degree\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.title(f'{dataset} Power Law')\n",
    "    plt.savefig(f'detection/experiments/{dataset}_power_law.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    return alpha, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ego_subgraph(g, vertices, order_k = 1):\n",
    "    ego_network = g.neighborhood(vertices=vertices, order=order_k, mode='out')\n",
    "    set_of_tuples = set(tuple(inner_list) for inner_list in ego_network)\n",
    "    flattened_set = list({element for tupl in set_of_tuples for element in tupl})\n",
    "    ego_subgraph = g.subgraph(flattened_set)\n",
    "    return ego_subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_output_dir = \"/sise/home/tommarz/hate_speech_detection/data/networks_data\"\n",
    "raw_graphs_dict_path = os.path.join(network_output_dir, \"raw_graphs_dict.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parler_all_users_path =  \"/sise/home/tommarz/hate_speech_detection/detection/outputs/parler/BertFineTuning/user_level/split_by_posts/no_text/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Reposts Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['echo_2', 'gab', 'parler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.5 s, sys: 2.76 s, total: 13.2 s\n",
      "Wall time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if os.path.exists(raw_graphs_dict_path):\n",
    "    raw_graphs_dict = pickle.load(open(raw_graphs_dict_path, 'rb'))\n",
    "elif \"raw_graphs_dict\" not in globals():\n",
    "    raw_graphs_dict = {d: get_reposts_graph(d) for d in datasets}\n",
    "    with open(raw_graphs_dict_path, 'wb') as f:\n",
    "        pickle.dump(raw_graphs_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create Datasets Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'echo_2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IGRAPH DNW- 10274 196981 -- echo_2\n",
      "+ attr: name (g), label (v), name (v), weight (e)\n"
     ]
    }
   ],
   "source": [
    "raw_g = raw_graphs_dict[dataset].copy()\n",
    "print(raw_g.summary())\n",
    "network_dataset_output_dir = os.path.join(network_output_dir, dataset)\n",
    "raw_network_path  = os.path.join(network_dataset_output_dir, \"raw_network.p\")\n",
    "network_with_singletons_path  = os.path.join(network_dataset_output_dir, \"network_with_singletons.p\")\n",
    "largest_cc_path  = os.path.join(network_dataset_output_dir, \"largest_cc.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Posts and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path_conf = path_confs[dataset]\n",
    "if dataset == 'parler':\n",
    "    preds_df = pd.read_parquet(parler_all_users_path)\n",
    "else:\n",
    "    preds_df = pd.read_parquet(dataset_path_conf['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>231597325</td>\n",
       "      <td>0.014254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>231597325</td>\n",
       "      <td>0.003655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>231597325</td>\n",
       "      <td>0.003499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>231597325</td>\n",
       "      <td>0.003180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>231597325</td>\n",
       "      <td>0.507950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17575990</th>\n",
       "      <td>2880670643</td>\n",
       "      <td>0.015104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17575991</th>\n",
       "      <td>2880670643</td>\n",
       "      <td>0.006758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17575992</th>\n",
       "      <td>2880670643</td>\n",
       "      <td>0.006917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17575993</th>\n",
       "      <td>2880670643</td>\n",
       "      <td>0.020877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17575994</th>\n",
       "      <td>2880670643</td>\n",
       "      <td>0.011098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17575995 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             user_id  predictions\n",
       "0          231597325     0.014254\n",
       "1          231597325     0.003655\n",
       "2          231597325     0.003499\n",
       "3          231597325     0.003180\n",
       "4          231597325     0.507950\n",
       "...              ...          ...\n",
       "17575990  2880670643     0.015104\n",
       "17575991  2880670643     0.006758\n",
       "17575992  2880670643     0.006917\n",
       "17575993  2880670643     0.020877\n",
       "17575994  2880670643     0.011098\n",
       "\n",
       "[17575995 rows x 2 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Doc2Vec (for GNN later on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7073, 100)\n"
     ]
    }
   ],
   "source": [
    "with open(dataset_path_conf['doc_vectors'], 'rb') as f:\n",
    "    doc2vec = pickle.load(f)\n",
    "docs_arr = np.array(list(doc2vec.values()))\n",
    "print(docs_arr.shape)\n",
    "# mean, std = docs_arr.mean(), docs_arr.std()\n",
    "# mean, std "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "231597325     0\n",
       "2190420108    0\n",
       "548007350     1\n",
       "113526237     0\n",
       "716664192     0\n",
       "             ..\n",
       "225298549     0\n",
       "460453341     0\n",
       "88994026      0\n",
       "187450820     0\n",
       "2974346781    1\n",
       "Name: label, Length: 1000, dtype: int64"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.read_csv(user_level_conf[dataset]['data_path'], sep='\\t').set_index('user_id')['label']\n",
    "y.index = y.index.astype(str)\n",
    "print(y.mean())\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7073"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_with_posts = preds_df['user_id'].unique()\n",
    "len(users_with_posts)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "set(users_with_posts) == (set(doc2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7073"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_with_posts_and_docs = set(users_with_posts).intersection(set(doc2vec))\n",
    "len(users_with_posts_and_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IGRAPH DNW- 10274 196981 -- echo_2\\n+ attr: name (g), label (v), name (v), weight (e)'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_g.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Singletons: 0\n"
     ]
    }
   ],
   "source": [
    "singletons = raw_g.vs[[index for index, degree in enumerate(raw_g.vs.degree()) if degree == 0]]\n",
    "print(f'# of Singletons: {len(singletons)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Singletons: 2211\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'IGRAPH DNW- 12485 196981 -- echo_2\\n+ attr: name (g), label (v), name (v), weight (e)'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singletons = list(set(users_with_posts_and_docs).difference(set(raw_g.vs['name'])))\n",
    "attributes={'label': [y.get(user_id, default=-1) for user_id in singletons]}\n",
    "print(f'# of Singletons: {len(singletons)}')\n",
    "\n",
    "g_with_singletons = raw_g.copy()\n",
    "g_with_singletons.summary()\n",
    "\n",
    "g_with_singletons.add_vertices(singletons, attributes={'label': [y.get(user_id, default=-1) for user_id in singletons]})\n",
    "g_with_singletons.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Labeled Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_nodes = g_with_singletons.vs.select(lambda v: v['label'] != -1)\n",
    "len(labeled_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7073\n"
     ]
    }
   ],
   "source": [
    "nodes_with_posts_and_docs = g_with_singletons.vs.select(lambda v: v['name'] in users_with_posts_and_docs)\n",
    "print(len(nodes_with_posts_and_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Filter out user (nodes) without any posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IGRAPH DNW- 7073 21409 -- echo_2\\n+ attr: name (g), label (v), name (v), weight (e)'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = g_with_singletons.subgraph(nodes_with_posts_and_docs)\n",
    "g.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 0.154)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_nodes = g.vs.select(lambda v: v['label'] != -1)\n",
    "len(labeled_nodes), np.mean(labeled_nodes['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "410"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_singletons = labeled_nodes[[index for index, degree in enumerate(labeled_nodes.degree()) if degree == 0]]\n",
    "len(labeled_singletons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Singletons: 2919\n"
     ]
    }
   ],
   "source": [
    "singletons = g.vs[[index for index, degree in enumerate(g.vs.degree()) if degree == 0]]\n",
    "print(f'# of Singletons: {len(singletons)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Weakly Connected Components: 3067\n",
      "Number of Strongly Connected Components: 6008\n",
      "Total number of Connected Components: 9075\n",
      "Size of Largest Weakly Connected Component: 3746\n"
     ]
    }
   ],
   "source": [
    "weakly_connected_components = g.components(mode=\"weak\")\n",
    "print(\"Number of Weakly Connected Components:\", len(weakly_connected_components))\n",
    "\n",
    "# Find strongly connected components\n",
    "strongly_connected_components = g.components(mode=\"strong\")\n",
    "print(\"Number of Strongly Connected Components:\", len(strongly_connected_components))\n",
    "\n",
    "print('Total number of Connected Components:', len(weakly_connected_components) + len(strongly_connected_components))\n",
    "\n",
    "# You can also explore the size of the largest component, or other properties\n",
    "largest_weakly_component = max(weakly_connected_components, key=len)\n",
    "print(\"Size of Largest Weakly Connected Component:\", len(largest_weakly_component))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17575995"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17575995"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds_df.query('`user_id` in @g.vs[\"name\"]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_preds_agg = preds_df.query('`user_id` in @g.vs[\"name\"]').groupby('user_id')['predictions'].agg(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.vs['doc2vec'] = [doc2vec[name] for name in g.vs['name']]\n",
    "# g.vs['doc2vec'] = [doc2vec.get(v['name'],  np.zeros(100)) for v in g.vs]\n",
    "g.vs['predictions'] = [np.array(node_preds_agg[name]) for name in g.vs['name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IGRAPH DNW- 3746 20728 -- echo_2\\n+ attr: name (g), doc2vec (v), label (v), name (v), predictions (v), weight (e)'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "largest_cc = get_largest_weak_cc(g)\n",
    "largest_cc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9805943"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds_df.query('`user_id` in @largest_cc.vs[\"name\"]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(532, 0.26127819548872183)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_nodes = largest_cc.vs.select(lambda v: v['label'] != -1)\n",
    "len(labeled_nodes), np.mean(labeled_nodes['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connected Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_of_cc(g):\n",
    "    weakly_connected_components = g.components(mode=\"weak\")\n",
    "    print(\"Number of Weakly Connected Components:\", len(weakly_connected_components))\n",
    "\n",
    "    # Find strongly connected components\n",
    "    strongly_connected_components = g.components(mode=\"strong\")\n",
    "    print(\"Number of Strongly Connected Components:\", len(strongly_connected_components))\n",
    "\n",
    "    print('Total number of Connected Components:', len(weakly_connected_components) + len(strongly_connected_components))\n",
    "\n",
    "    # You can also explore the size of the largest component, or other properties\n",
    "    largest_weakly_component = max(weakly_connected_components, key=len)\n",
    "    print(\"Size of Largest Weakly Connected Component:\", len(largest_weakly_component))\n",
    "    return len(weakly_connected_components) + len(strongly_connected_components), len(strongly_connected_components), len(weakly_connected_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Weakly Connected Components: 3067\n",
      "Number of Strongly Connected Components: 6008\n",
      "Total number of Connected Components: 9075\n",
      "Size of Largest Weakly Connected Component: 3746\n"
     ]
    }
   ],
   "source": [
    "num_cc, _, _ = get_num_of_cc(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Weakly Connected Components: 1\n",
      "Number of Strongly Connected Components: 2761\n",
      "Total number of Connected Components: 2762\n",
      "Size of Largest Weakly Connected Component: 3746\n"
     ]
    }
   ],
   "source": [
    "num_cc, _, _ = get_num_of_cc(largest_cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Coefficient"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Calculate clustering coefficient\n",
    "# For directed graphs, you can use \"average\" to get the average of in and out coefficients\n",
    "for name, g in raw_graphs_dict.items():\n",
    "    print(name)\n",
    "    clustering_coefficient = g.transitivity_avglocal_undirected()\n",
    "    print(\"Average Clustering Coefficient:\", clustering_coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2088079566514251"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.transitivity_avglocal_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19628344812349038"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "largest_cc.transitivity_avglocal_undirected()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Modularity"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "g.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "communities = g.community_infomap()\n",
    "\n",
    "# Print the community membership of each vertex\n",
    "# print(\"Community membership:\", communities.membership)\n",
    "\n",
    "# Calculate and print the modularity\n",
    "modularity_value = g.modularity(communities)\n",
    "print(\"Modularity of the graph:\", modularity_value)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "communities = largest_cc.community_infomap()\n",
    "\n",
    "# Print the community membership of each vertex\n",
    "# print(\"Community membership:\", communities.membership)\n",
    "\n",
    "# Calculate and print the modularity\n",
    "modularity_value = largest_cc.modularity(communities)\n",
    "print(\"Modularity of the graph:\", modularity_value)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Find weakly connected components\n",
    "for name, g in raw_graphs_dict.items():\n",
    "    print(name)\n",
    "    weakly_connected_components = g.components(mode=\"weak\")\n",
    "    print(\"Number of Weakly Connected Components:\", len(weakly_connected_components))\n",
    "\n",
    "    # Find strongly connected components\n",
    "    strongly_connected_components = g.components(mode=\"strong\")\n",
    "    print(\"Number of Strongly Connected Components:\", len(strongly_connected_components))\n",
    "\n",
    "    print('Total number of Connected Components:', len(weakly_connected_components) + len(strongly_connected_components))\n",
    "\n",
    "    # You can also explore the size of the largest component, or other properties\n",
    "    largest_weakly_component = max(weakly_connected_components, key=len)\n",
    "    print(\"Size of Largest Weakly Connected Component:\", len(largest_weakly_component))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Raw (Full) Network and Largest (Weakly) Connected Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(network_dataset_output_dir):\n",
    "    os.mkdir(network_dataset_output_dir)\n",
    "with open(raw_network_path, 'wb') as f:\n",
    "    pickle.dump(g, f)\n",
    "with open(largest_cc_path, 'wb') as f:\n",
    "    pickle.dump(largest_cc, f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Filter by mininum edge weight"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "edge_weights = np.array(g.es['weight'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "percentiles=[1, 5, 10, 25, 50, 75, 9, 95, 99]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "reposts_df_weight_stats = {q: p for q,p in zip(percentiles, np.percentile(edge_weights, q=percentiles))}\n",
    "print(reposts_df_weight_stats)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "filtered_graphs_dict = {}\n",
    "largest_component_dict = {}\n",
    "for d in datasets:\n",
    "    print(d)\n",
    "    g = get_reposts_graph(d)\n",
    "    edge_weights = np.array(g.es['weight'])\n",
    "    min_weight = edge_weights.mean()\n",
    "    h = get_reposts_graph(d, min_weight, graphs_dict)\n",
    "    filtered_graphs_dict[d]  = h\n",
    "    print(h.summary())\n",
    "    largest_component = get_largest_weak_cc(h)\n",
    "    print(largest_component.summary())\n",
    "    largest_component_dict[d] = largest_component"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "filtered_graphs_dict_path = os.path.join(sna_dir, 'filtered_graphs_dict.pkl')\n",
    "filtered_graphs_dict_path"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'filtered_graphs_dict' in locals()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "os.path.exists(filtered_graphs_dict_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "if os.path.exists(filtered_graphs_dict_path):\n",
    "    filtered_graphs_dict = pickle.load(open(filtered_graphs_dict_path, 'rb'))\n",
    "elif 'filtered_graphs_dict' in locals():\n",
    "    with open(filtered_graphs_dict_path, 'wb') as f:\n",
    "        pickle.dump(filtered_graphs_dict, f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "largest_component_dict_path = os.path.join(sna_dir, 'largest_component_dict.pkl')\n",
    "largest_component_dict_path"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "if os.path.exists(largest_component_dict_path):\n",
    "    largest_component_dict = pickle.load(open(largest_component_dict_path, 'rb'))\n",
    "elif 'largest_component_dict' in locals():\n",
    "    with open(largest_component_dict_path, 'wb') as f:\n",
    "        pickle.dump(largest_component_dict, f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "g = filtered_graph_dict[dataset]\n",
    "g = graphs_dict[dataset]\n",
    "g.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "labeled_nodes = get_labeled_nodes(g=g)\n",
    "node_labels = np.array([v['label'] for v in labeled_nodes])\n",
    "print(len(node_labels), node_labels.mean())\n",
    "ego_subgraph = get_ego_subgraph(g, labeled_nodes, order_k=3)\n",
    "ego_subgraph.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "largest_weak_cc = get_largest_weak_cc(ego_subgraph)\n",
    "largest_weak_cc.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "labeled_nodes = get_labeled_nodes(g=largest_weak_cc)\n",
    "node_labels = np.array([v['label'] for v in labeled_nodes])\n",
    "print(len(node_labels), node_labels.mean())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "echo_2pkl_name = 'echo_2_ego_network_order_3_largest_weak_cc.pkl'\n",
    "full_path = os.path.join(sna_dir, echo_2pkl_name)\n",
    "full_path"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "largest_weak_cc.save(full_path, format='pickle')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def plot_igraph(g, dataset=dataset, title=None, **visual_style):\n",
    "    # scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    #scaler.fit_transform(np.expand_dims(g.es['weight'], 1)).flatten() + 0.01)\n",
    "    label_color_map = {\n",
    "        0: 'red',\n",
    "        1: 'green',\n",
    "        -1: 'gray'\n",
    "    }\n",
    "    g.vs['color'] = [label_color_map[label] for label in g.vs['label']]\n",
    "    # node_names = [name for name in g.vs['name']]\n",
    "    fig, ax = plt.subplots(figsize=(40,20), dpi=300)\n",
    "    title = title if title else f'{dataset} network'\n",
    "    # plt.title(title)\n",
    "    # print(visual_style)\n",
    "    ig.plot(g, target=ax, **visual_style)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def softmax(x, temperature=1):\n",
    "    e_x = np.exp(x - np.max(x))  # subtracting max for numerical stability\n",
    "    return (e_x/temperature)/ e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_igraph_visual_style(g, temperature=1):\n",
    "    visual_style = {}\n",
    "    visual_style['vertex_size'] = softmax(g.degree(), temperature) + 0.1\n",
    "    visual_style['vertex_label'] = None\n",
    "    visual_style['margin'] = 20\n",
    "    visual_style['edge_color'] = 'gray'\n",
    "    visual_style['edge_width'] = (np.log10(np.array(g.es['weight']) + 0)).tolist()\n",
    "    visual_style['layout'] = g.layout_fruchterman_reingold() #weights=visual_style['edge_width'])\n",
    "    visual_style['vertex_frame_color'] = 'white'\n",
    "    # visual_style['edge_arrow_size'] = (np.array(visual_style['edge_width'])/2).tolist()\n",
    "    # visual_style['edge_arrow_width'] = (np.array(visual_style['edge_width'])/2).tolist()\n",
    "    return visual_style"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Full Network"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "g = graphs_dict[dataset]\n",
    "g.vcount(), g.ecount()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "g.degree_distribution().mean, g.degree_distribution().sd, g.degree_distribution().n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.percentile(g.degree(), 99)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(20,6))\n",
    "sns.kdeplot(data=g.degree(), ax=axs[0])\n",
    "sns.kdeplot(x=g.degree(mode='in'), ax=axs[1])\n",
    "sns.kdeplot(x=g.degree(mode='out'), ax=axs[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "plot_igraph(g, **get_igraph_visual_style(g))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Largest Weakly Component"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "for d in datasets:\n",
    "    print(d)\n",
    "    largest_weakly_component = largest_component_dict[d]\n",
    "    print(largest_weakly_component.summary())\n",
    "\n",
    "    _, degree_counts = np.unique(g.degree(), return_counts=True)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20,6))\n",
    "    sns.countplot(x=largest_weakly_component.degree(), ax=axs[0])\n",
    "    sns.countplot(x=largest_weakly_component.degree(mode='in'), ax=axs[1])\n",
    "    sns.countplot(x=largest_weakly_component.degree(mode='out'), ax=axs[2])\n",
    "    # axs[2].set_xticks(ticks=axs[2].get_xticks(), labels=ticks=axs[2].get_xticklabls(), rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "    plot_igraph(largest_weakly_component, title=f'{dataset} largest connected component', **get_igraph_visual_style(largest_weakly_component, temperature=0.75))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Ego Network"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "order_k = 1\n",
    "ego_network = g.neighborhood(vertices=labeled_nodes, order=order_k, mode='out')\n",
    "set_of_tuples = set(tuple(inner_list) for inner_list in ego_network)\n",
    "flattened_set = list({element for tupl in set_of_tuples for element in tupl})\n",
    "ego_subgraph = g.subgraph(flattened_set)\n",
    "ego_subgraph.vcount(), ego_subgraph.ecount()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "visual_style = {}\n",
    "visual_style['vertex_size'] = 0.5\n",
    "visual_style['vertex_label'] = None\n",
    "visual_style['layout'] = ego_subgraph.layout('kk')\n",
    "# visual_style['bbox'] = (300, 300)\n",
    "visual_style['margin'] = 20\n",
    "# kwds['target'] = ax\n",
    "visual_style['edge_color'] = 'black'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "title = f'{dataset} ego network of labeled nodes (order of {order_k}) with min edge weight: 2'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "plot_igraph(ego_subgraph, title, **visual_style)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "layout = ego_subgraph.layout('kk')  # Kamada-Kawai layout for nicer visualization\n",
    "# layout = ego_subgraph.layout_fruchterman_reingold()\n",
    "\n",
    "node_labels = [y.loc[e['name']] if e['name'] in y.index else -1 for e in ego_subgraph.vs]\n",
    "ego_subgraph.vs['label'] = node_labels \n",
    "\n",
    "label_color_map = {\n",
    "    0: 'red',\n",
    "    1: 'green',\n",
    "    -1: 'gray'\n",
    "}\n",
    "node_colors = [label_color_map[label] for label in ego_subgraph.vs['label']]\n",
    "node_names = [name for name in g.vs['name']]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(1, 5))\n",
    "edge_widths = scaler.fit_transform(np.array([w for w in ego_subgraph.es['weight']]).reshape(-1, 1)).flatten()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(40,20))\n",
    "# save_dir = 'detection/experiments/sna'\n",
    "title = f'{dataset} ego network of labeled nodes (order of {order_k}) with min edge weight: {min_weight}'\n",
    "# save_path = os.path.join(save_dir, f'{title}.pdf')\n",
    "plt.title(title)\n",
    "ig.plot(ego_subgraph, layout=layout, arrow_size=1, target=ax, vertex_size=0.5, vertex_color=node_colors, edge_color='black', bbox=(0, 0, 1000, 1000), vertex_label=None, edge_width=edge_widths)\n",
    "# ig.plot(g, layout=layout, target=ax, margin=50, vertex_color=node_colors, edge_color='black', bbox=(0, 0, 1000, 1000), edge_width=edge_widths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from igraph import Graph, plot\n",
    "\n",
    "# Example: Creating a graph with weighted edges\n",
    "edges = [(0, 1), (0, 2), (0, 3), (1, 4), (1, 5), (2, 6), (2, 7), (3, 8), (3, 9)]\n",
    "weights = [1, 2, 3, 4, 5, 1, 2, 3, 4]  # Example weights for each edge\n",
    "\n",
    "g = Graph(edges=edges, directed=False)\n",
    "g.es['weight'] = weights\n",
    "\n",
    "# Assuming you're using weights as widths directly for simplicity, adjust as needed\n",
    "edge_widths = [weight for weight in g.es['weight']]\n",
    "\n",
    "# Calculate layout\n",
    "layout = g.layout_fruchterman_reingold()\n",
    "\n",
    "# Plot the graph with specified edge widths\n",
    "ig.plot(g, layout=layout, bbox=(400, 400), margin=20, edge_width=edge_widths)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "layout = g.layout_fruchterman_reingold()\n",
    "\n",
    "node_labels = [y.loc[e['name']] if e['name'] in y.index else -1 for e in g.vs]\n",
    "g.vs['label'] = node_labels \n",
    "\n",
    "label_color_map = {\n",
    "    0: 'red',\n",
    "    1: 'green',\n",
    "    -1: 'gray'\n",
    "}\n",
    "node_colors = [label_color_map[label] for label in g.vs['label']]\n",
    "node_names = [name for name in g.vs['name']]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(1, 10))\n",
    "edge_widths = scaler.fit_transform(np.array([w for w in g.es['weight']]).reshape(-1, 1)).flatten()\n",
    "\n",
    "g.es['width'] = [weight / max_weight * 10 for weight in g.es['weight']]  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "fig, ax = plt.subplots(figsize=(40,20))\n",
    "save_dir = 'detection/experiments/sna'\n",
    "title = f'{dataset} with min edge weight = {min_weight}'\n",
    "save_path = os.path.join(save_dir, f'{title}.pdf')\n",
    "plt.title(title)\n",
    "ig.plot(g, layout=layout, target=ax, vertex_size=0.5, vertex_color=node_colors, edge_color='black', bbox=(0, 0, 1000, 1000), vertex_label=None, edge_width=edge_widths)\n",
    "# ig.plot(g, layout=layout, target=ax, margin=50, vertex_color=node_colors, edge_color='black', bbox=(0, 0, 1000, 1000), edge_width=edge_widths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import igraph as ig\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Create a weighted graph with labels\n",
    "g = ig.Graph(directed=False)\n",
    "\n",
    "# Add vertices\n",
    "g.add_vertices(5)  # Adding 5 vertices for example\n",
    "\n",
    "# Add edges with weights\n",
    "g.add_edges([(0, 1), (1, 2), (2, 3), (3, 4), (4, 0)])\n",
    "g.es['weight'] = [1, 2, 3, 4, 5]  # Example weights\n",
    "\n",
    "# Add labels to vertices\n",
    "g.vs['label'] = ['A', 'B', 'C', 'D', 'E']  # Example labels\n",
    "\n",
    "# Step 2: Assign colors based on labels\n",
    "label_color_dict = {'A': 'red', 'B': 'green', 'C': 'blue', 'D': 'yellow', 'E': 'orange'}\n",
    "g.vs['color'] = [label_color_dict[label] for label in g.vs['label']]\n",
    "\n",
    "# Step 3: Adjust edge thickness based on weights (normalize for visualization)\n",
    "max_weight = max(g.es['weight'])\n",
    "g.es['width'] = [weight / max_weight * 10 for weight in g.es['weight']]  # Adjust multiplier as needed\n",
    "\n",
    "# Step 4: Draw the graph\n",
    "visual_style = {}\n",
    "visual_style['vertex_size'] = 20\n",
    "visual_style['vertex_color'] = g.vs['color']\n",
    "visual_style['vertex_label'] = g.vs['label']\n",
    "visual_style['edge_width'] = g.es['width']\n",
    "visual_style['bbox'] = (300, 300)\n",
    "visual_style['margin'] = 20\n",
    "\n",
    "ig.plot(g, **visual_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find weakly connected components\n",
    "d = {}\n",
    "for name, g in filtered_igraph_dict.items():\n",
    "    print(name)\n",
    "    weakly_connected_components = g.components(mode=\"weak\")\n",
    "    print(\"Number of Weakly Connected Components:\", len(weakly_connected_components))\n",
    "    # Find strongly connected components\n",
    "    strongly_connected_components = g.components(mode=\"strong\")\n",
    "    print(\"Number of Strongly Connected Components:\", len(strongly_connected_components))\n",
    "\n",
    "    print('Total number of Connected Components:', len(weakly_connected_components) + len(strongly_connected_components))\n",
    "\n",
    "    # You can also explore the size of the largest component, or other properties\n",
    "    largest_weakly_component = max(weakly_connected_components, key=len)\n",
    "    print(\"Size of Largest Weakly Connected Component:\", len(largest_weakly_component))\n",
    "    \n",
    "    d[name] = [len(weakly_connected_components), len(strongly_connected_components), len(weakly_connected_components) + len(strongly_connected_components), len(largest_weakly_component)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find weakly connected components\n",
    "for name, g in raw_graphs_dict.items():\n",
    "    print(name)\n",
    "    degrees = g.degree()\n",
    "\n",
    "    # Count the number of vertices with degree 0 (singletons)\n",
    "    singletons = degrees.count(0)\n",
    "\n",
    "    print(\"Number of singletons:\", singletons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = g.degree()\n",
    "in_degree_lst = g.degree(mode='in')\n",
    "out_degree_lst = g.degree(mode='out')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "calc_power_law_exp(out_degree_lst)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "calc_power_law_exp(in_degree_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, C = calc_power_law_exp(degrees)\n",
    "C, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centrality Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Centrality Measures\n",
    "for name, g in filtered_igraph_dict.items():\n",
    "    print(name)\n",
    "    degree_centrality = g.degree()\n",
    "    betweenness_centrality = g.closeness()\n",
    "    closeness_centrality = g.betweenness()\n",
    "\n",
    "    mean_degree_centrality, mean_betweenness_centrality, mean_closeness_centrality = np.mean([degree_centrality, betweenness_centrality, closeness_centrality], axis=1)\n",
    "    print(mean_degree_centrality, mean_betweenness_centrality, mean_closeness_centrality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if the graph is connected\n",
    "if g.is_connected():\n",
    "    avg_path_length = g.average_path_length()\n",
    "    print(\"Average shortest path length:\", avg_path_length)\n",
    "else:\n",
    "    print(\"Graph is not connected. Average path length is undefined for the whole graph.\")\n",
    "#     for c in g.components(mode=\"strong\"):\n",
    "#         h = g.subgraph(c)\n",
    "#         if h.vcount() < 2:\n",
    "#             continue\n",
    "#         print(h.vcount(), h.ecount())\n",
    "#         # Centrality Measures\n",
    "#         degree_centrality = h.degree()\n",
    "#         betweenness_centrality = h.closeness()\n",
    "#         closeness_centrality = h.betweenness()\n",
    "\n",
    "#         mean_degree_centrality, mean_betweenness_centrality, mean_closeness_centrality = np.mean([degree_centrality, betweenness_centrality, closeness_centrality], axis=1)\n",
    "#         print(mean_degree_centrality, mean_betweenness_centrality, mean_closeness_centrality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truth Social"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'truth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_weight = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# truth_reposts_graph = get_reposts_graph('truth')\n",
    "truth_reposts_path = path_confs['truth']['reposts']\n",
    "truth_reposts_df = pd.read_csv(reposts_path, sep='\\t', names=['source', 'target', 'retruths_list', 'weight'], skiprows=1)\n",
    "truth_filtered_reposts_df = truth_reposts_df.query('`weight`>@min_weight and source!=target')\n",
    "truth_filtered_reposts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ig.Graph.TupleList(truth_filtered_reposts_df.values, directed=True, edge_attrs=\"weight\")\n",
    "print(g.vcount(), g.ecount())\n",
    "g.vs.select(_degree=0).delete()\n",
    "g.simplify(multiple=True, loops=True)\n",
    "print(g.vcount(), g.ecount())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(40,20))\n",
    "save_dir = 'detection/experiments/sna'\n",
    "title = f'{dataset} with min edge weight = {min_weight}'\n",
    "save_path = os.path.join(save_dir, f'{title}.pdf')\n",
    "plt.title(title)\n",
    "ig.plot(g, target=ax, arrow_size=0.5, edge_size=1, vertex_size=7, vertex_color='lightblue', edge_color='gray', bbox=(0, 0, 600, 600))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate clustering coefficient\n",
    "# For directed graphs, you can use \"average\" to get the average of in and out coefficients\n",
    "clustering_coefficient = g.transitivity_avglocal_undirected()\n",
    "print(\"Average Clustering Coefficient:\", clustering_coefficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connected Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find weakly connected components\n",
    "weakly_connected_components = g.components(mode=\"weak\")\n",
    "print(\"Number of Weakly Connected Components:\", len(weakly_connected_components))\n",
    "\n",
    "# Find strongly connected components\n",
    "strongly_connected_components = g.components(mode=\"strong\")\n",
    "print(\"Number of Strongly Connected Components:\", len(strongly_connected_components))\n",
    "\n",
    "print('Total number of Connected Components:', len(weakly_connected_components) + len(strongly_connected_components))\n",
    "\n",
    "# You can also explore the size of the largest component, or other properties\n",
    "largest_weakly_component = max(weakly_connected_components, key=len)\n",
    "print(\"Size of Largest Weakly Connected Component:\", len(largest_weakly_component))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = g.degree()\n",
    "\n",
    "# Count the number of vertices with degree 0 (singletons)\n",
    "singletons = degrees.count(0)\n",
    "\n",
    "print(\"Number of singletons:\", singletons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = g.degree()\n",
    "in_degree_lst = g.degree(mode='in')\n",
    "out_degree_lst = g.degree(mode='out')\n",
    "degrees[0], degrees[0], degrees[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'degrees' with your actual data containing node degrees\n",
    "def calc_power_law_exp(degrees):\n",
    "\n",
    "    # Convert the degrees to numpy arrays\n",
    "    x = np.array(degrees)\n",
    "\n",
    "    # Count the frequency of each degree\n",
    "    degree_counts = np.bincount(x)\n",
    "\n",
    "    # Remove the zero-degree entries\n",
    "    x = np.nonzero(degree_counts)[0]\n",
    "    y = degree_counts[x]\n",
    "\n",
    "    # Perform the curve fitting\n",
    "    popt, _ = curve_fit(power_law, x, y)\n",
    "    alpha, C = popt\n",
    "    \n",
    "    plt.scatter(x, y, label=\"Data\")\n",
    "    plt.plot(x, power_law(x, alpha, C), color='red', label=f\"Power-law fit (alpha={alpha:.2f})\")\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel(\"Degree\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.title(f'{dataset} Power Law')\n",
    "    plt.savefig(f'detection/experiments/{dataset}_power_law.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    return alpha, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, C = calc_power_law_exp(degrees)\n",
    "C, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centrality Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(g.degree()), np.mean(g.closeness()), np.mean(g.betweenness())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Centrality Measures\n",
    "degree_centrality = g.degree()\n",
    "betweenness_centrality = g.closeness()\n",
    "closeness_centrality = g.betweenness()\n",
    "\n",
    "mean_degree_centrality, mean_betweenness_centrality, mean_closeness_centrality = np.mean([degree_centrality, betweenness_centrality, closeness_centrality], axis=1)\n",
    "mean_degree_centrality, mean_betweenness_centrality, mean_closeness_centrality"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Path Analysis\n",
    "average_shortest_path_length = nx.average_shortest_path_length(g)\n",
    "print(f\"\\nAverage Shortest Path Length: {average_shortest_path_length:.2f}\")\n",
    "# Add more analyses as needed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if the graph is connected\n",
    "if g.is_connected():\n",
    "    avg_path_length = g.average_path_length()\n",
    "    print(\"Average shortest path length:\", avg_path_length)\n",
    "else:\n",
    "    print(\"Graph is not connected. Average path length is undefined for the whole graph.\")\n",
    "    for c in g.components(mode=\"strong\"):\n",
    "        h = g.subgraph(c)\n",
    "        if h.vcount() < 2:\n",
    "            continue\n",
    "        print(h.vcount(), h.ecount())\n",
    "        # Centrality Measures\n",
    "        degree_centrality = h.degree()\n",
    "        betweenness_centrality = h.closeness()\n",
    "        closeness_centrality = h.betweenness()\n",
    "\n",
    "        mean_degree_centrality, mean_betweenness_centrality, mean_closeness_centrality = np.mean([degree_centrality, betweenness_centrality, closeness_centrality], axis=1)\n",
    "        print(mean_degree_centrality, mean_betweenness_centrality, mean_closeness_centrality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregative Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict, cross_val_score, KFold, StratifiedKFold, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_list = [accuracy_score, precision_score, recall_score, f1_score, roc_auc_score]\n",
    "scoring_names = ['_'.join(f.__name__.split('_')[:-1]) for f in [accuracy_score, precision_score, recall_score, f1_score, roc_auc_score]]\n",
    "scoring_dict = {n:f for n, f in zip(scoring_names, scoring_list)}\n",
    "# scoring_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(labeled_nodes['label'])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_th = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hs_count(v, th=0):\n",
    "    return (v['predictions']>=th).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_cc.vs['hs_count'] = [get_hs_count(v, post_th) for v in largest_cc.vs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(labeled_nodes['hs_count'], columns=['hs_count'], index=labeled_nodes['name'])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = clf.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y, probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pipe = make_pipeline(StandardScaler(), SimpleImputer(strategy='constant', fill_value=0), LogisticRegression(random_state=echo_seed))\n",
    "pipe = make_pipeline(StandardScaler(), SimpleImputer(strategy='mean'), LogisticRegression(random_state=seed))\n",
    "pipe.set_output(transform='pandas')\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'logisticregression__C': np.logspace(0, -3, 20),\n",
    "    'logisticregression__class_weight': ['balanced']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(pipe, param_grid, cv=StratifiedKFold(shuffle=True, random_state=seed), scoring=scoring_names, return_train_score=True, refit='f1', n_jobs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict_fixed = get_best_results_from_gs(gs)\n",
    "for name, df in results_dict_fixed.items():\n",
    "    print(name)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m, s in results_dict_fixed['test'].astype(float).values:\n",
    "    print(f'{m:.3f}+-{s:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relational Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relational_feats(v, post_th=0.5):\n",
    "    hs_count = v['hs_count']\n",
    "    following_hs_count = np.array(largest_cc.vs[largest_cc.neighbors(v, mode='in')]['hs_count'])\n",
    "    following_mean_hs_count = following_hs_count.mean() if following_hs_count.size>0 else 0\n",
    "    followees_hs_count = np.array(largest_cc.vs[largest_cc.neighbors(v, mode='out')]['hs_count'])\n",
    "    followees_mean_hs_count = followees_hs_count.mean() if followees_hs_count.size>0 else 0\n",
    "    return hs_count, following_mean_hs_count, followees_mean_hs_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(np.array([get_relational_feats(v) for v in labeled_nodes]), columns=['hs_count', 'following_mean_hs_count', 'followees_mean_hs_count'], index=labeled_nodes['name'])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(labeled_nodes['label'])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = clf.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y, probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pipe = make_pipeline(StandardScaler(), SimpleImputer(strategy='constant', fill_value=0), LogisticRegression(random_state=echo_seed))\n",
    "pipe = make_pipeline(StandardScaler(), SimpleImputer(strategy='mean'), LogisticRegression(random_state=seed))\n",
    "pipe.set_output(transform='pandas')\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'logisticregression__C': np.logspace(0, -3, 20),\n",
    "    'logisticregression__class_weight': ['balanced']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(pipe, param_grid, cv=StratifiedKFold(shuffle=True, random_state=seed), scoring=scoring_names, return_train_score=True, refit='f1', n_jobs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict_relat = get_best_results_from_gs(gs)\n",
    "for name, df in results_dict_fixed.items():\n",
    "    print(name)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m, s in results_dict_relat['test'].astype(float).values:\n",
    "    print(f'{m:.3f}+-{s:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_hs_stats(v, percentiles=np.array([1,5,10,25,50,75,90,95,99])):\n",
    "    return np.array([v['predictions'].mean(), v['predictions'].std()] + np.percentile(v['predictions'], q=percentiles).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles=np.array([1,5,10,25,50,75,90,95,99])\n",
    "# percentiles/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(np.array([get_hs_stats(v) for v in labeled_nodes]), columns=['mean', 'std'] + [f'{p}%' for p in percentiles], index=labeled_nodes['name'])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pipe = make_pipeline(StandardScaler(), SimpleImputer(strategy='constant', fill_value=0), LogisticRegression(random_state=echo_seed))\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression(random_state=seed))\n",
    "pipe.set_output(transform='pandas')\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'logisticregression__C': np.logspace(0, -3, 20),\n",
    "    'logisticregression__class_weight': ['balanced']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(pipe, param_grid, cv=StratifiedKFold(shuffle=True, random_state=seed), scoring=scoring_names, return_train_score=True, refit='f1', n_jobs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict_dynamic = get_best_results_from_gs(gs)\n",
    "for name, df in results_dict_dynamic.items():\n",
    "    print(name)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for m, s in results_dict_dynamic['test'].astype(float).values:\n",
    "    print(f'{m:.3f}+-{s:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overleaf = ''\n",
    "for m, s in results_dict_dynamic['test'].astype(float).values:\n",
    "    overleaf += (f'${m:.3f} \\pm {s:.3f}$ & ')\n",
    "print(overleaf[:-3] + '\\\\\\\\')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Degroots Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "def fit_degroots_diffusion(nx_network, seed_hate_users, self_loops_dict,\n",
    "                           iterations=2, initial_belief=1, fix_seed_haters_belief=False, verbose=False):\n",
    "    \"\"\"\n",
    "    running a diffusion model as suggested in https://github.com/manoelhortaribeiro/HatefulUsersTwitter/blob/master/preprocessing/5_get_diffusion_graph.py\n",
    "\n",
    "    :param nx_network: networkX object. The social network to run the model over\n",
    "    :param seed_hate_users: list or set. the seed (original) hate users\n",
    "    :param self_loops_dict: dict. a dictionary where key is a username and value is the weight of self loop (this is\n",
    "    the total number of posts per user)\n",
    "    :param iterations: int. Default: 2. number of iterations to run the difussion mode (usually in te [1,5] range)\n",
    "    :param initial_belief: float. Default: 1. the initial belief values of the seed hate users. In most cases this is 1\n",
    "    :param fix_seed_haters_belief: bool. Default: False. whether or not to set the value of the hate users as the\n",
    "    initial_belief. This \"help\" propogate the hate and keep the seed hate users as haters over all iterations\n",
    "    :param verbose: bool. whether to print information along the prccess\n",
    "    :return: dict. a dictionary where key is the username and value is the belief value of the user at the end of the\n",
    "    process\n",
    "\n",
    "    Example:\n",
    "    input_for_networkx = [('A', 'B', 4),\n",
    "                          ('B', 'C', 3),\n",
    "                          ('C', 'A', 1), ('C', 'B', 6),\n",
    "                          ('D', 'B', 2), ('D', 'C', 4)]\n",
    "    social_network = nx.DiGraph()\n",
    "    social_network.add_weighted_edges_from(input_for_networkx)\n",
    "    seed_hate_users = ('A', 'B')\n",
    "    self_loops_dict = {'A': 6, 'B': 7, 'C': 0, 'D': 2}\n",
    "    iterations=2\n",
    "    initial_belief = 1.0\n",
    "    fix_seed_haters_belief = False\n",
    "    final_belief_dict = fit_degroots_diffusion(social_network, seed_hate_users, self_loops_dict, iterations,\n",
    "                                               initial_belief, fix_seed_haters_belief)\n",
    "    \"Out of the 2 seed hate users, 2 are found in the network\"\n",
    "    final_belief_dict\n",
    "    {'A': 0.8800000000000001, 'B': 0.7900000000000001, 'C': 0.7428571428571429, 'D': 0.7375}\n",
    "    \"\"\"\n",
    "    # NOTE!!! There are 1994 hate-users which ARE NOT in the network (since they only commented and did not echo anyone)\n",
    "\n",
    "    hate_users_in_network = [h for h in seed_hate_users if h in nx_network.nodes()]\n",
    "    not_hate_users = list(set(nx_network.nodes()).difference(set(hate_users_in_network)))\n",
    "    node_list = hate_users_in_network + not_hate_users\n",
    "    # inverse the network, since diffusion moves from the writter to the reader + adding self-loop\n",
    "    social_network_reversed = nx_network.reverse(copy=True)\n",
    "    for n in social_network_reversed.nodes():\n",
    "        if n in self_loops_dict:\n",
    "            social_network_reversed.add_edge(n, n, weight=self_loops_dict[n])\n",
    "        else:\n",
    "            social_network_reversed.add_edge(n, n, weight=0)\n",
    "    # the (i, j) place will have the weight of the edge between node i and j\n",
    "    transition_matrix = nx.adjacency_matrix(social_network_reversed, nodelist=node_list).asfptype()\n",
    "    # we need to transpose the matrix since now each column represents the INPUT arrows to each node and is sum to 1\n",
    "    # it is a bit confusing, but take a look at the example above and then it will make sense\n",
    "    transition_matrix = transition_matrix.transpose(copy=True)\n",
    "\n",
    "    # normalization, the short way...\n",
    "    rows_sum = np.sum(transition_matrix, axis=1).tolist()\n",
    "    inverse_row_sum = [1 / i[0] if i[0] > 0 else 0 for i in rows_sum]\n",
    "    inverse_row_sum_as_csr = sparse.csr_matrix(inverse_row_sum).transpose()\n",
    "    transition_matrix = transition_matrix.multiply(inverse_row_sum_as_csr)\n",
    "\n",
    "    beliefs = np.zeros(len(node_list))\n",
    "    beliefs[:len(hate_users_in_network)] = initial_belief\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        out = transition_matrix.dot(beliefs)\n",
    "        beliefs = out\n",
    "        # in case fix_seed_haters_belief is set to True, in each cycle the beliefs of the haters is set back to 1\n",
    "        if fix_seed_haters_belief:\n",
    "            beliefs[:len(hate_users_in_network)] = initial_belief\n",
    "    final_beliefs_dict = dict()\n",
    "    for node, belief in zip(node_list, beliefs):\n",
    "        final_beliefs_dict[node] = float(belief)\n",
    "    # sorting the dict by value\n",
    "    final_beliefs_dict = dict(sorted(final_beliefs_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "    if verbose:\n",
    "        print(f\"Out of the {len(seed_hate_users)} seed hate users, {len(hate_users_in_network)} are in the network\")\n",
    "    return final_beliefs_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "import pickle\n",
    "\n",
    "def get_doc_vectors(dataset):\n",
    "    if dataset in doc_vectors_dict:\n",
    "        return doc_vectors_dict[dataset]\n",
    "    doc_vectors = pickle.load(open(path_confs[dataset]['doc_vectors'], \"rb\"))\n",
    "    doc_vectors_dict[dataset] = doc_vectors\n",
    "    return doc_vectors\n",
    "\n",
    "def get_user_labels(dataset):\n",
    "    if dataset in labeled_nodes_dict:\n",
    "        return labeled_nodes_dict[dataset]\n",
    "    user2label_path = user_level_conf[dataset][\"data_path\"]\n",
    "    sep = \",\"\n",
    "    if user2label_path.endswith(\"tsv\"):\n",
    "        sep = \"\\t\"\n",
    "    y = pd.read_csv(user2label_path, sep=sep, index_col=[0]).squeeze()\n",
    "    y.index = y.index.astype('str')\n",
    "    labeled_nodes_dict[dataset] = y\n",
    "    return y\n",
    "\n",
    "def get_reposts_graph(dataset, min_weight=1):\n",
    "    if dataset in graphs_dict:\n",
    "        g = graphs_dict[dataset]\n",
    "        filtered_edges = g.es.select(weight_ge=min_weight)\n",
    "        return g.subgraph_edges(filtered_edges)\n",
    "    reposts_path = path_confs[dataset]['reposts']\n",
    "    if reposts_path.endswith('.txt'):\n",
    "        reposts_df = pd.read_csv(reposts_path, sep='\\t', header=None, names=['source', 'target', 'weight'])\n",
    "    elif reposts_path.endswith('.tsv'):\n",
    "        if dataset == 'truth':\n",
    "            pd.read_csv(reposts_path, sep='\\t', names=['source', 'target', 'retruths_list', 'weight'], skiprows=1)\n",
    "        else:\n",
    "            reposts_df = pd.read_csv(reposts_path, sep='\\t', names=['source', 'target', 'weight'], skiprows=1)\n",
    "    elif reposts_path.endswith('.csv'):\n",
    "        reposts_df = pd.read_csv(reposts_path, header=None, names=['source', 'target', 'weight'])\n",
    "    else:\n",
    "        reposts_edge_dict = pickle.load(open(reposts_path, \"rb\"))\n",
    "        reposts_edge_list = [[k[0], k[1], v] for k,v in tqdm(reposts_edge_dict.items())]\n",
    "        reposts_df = pd.DataFrame(reposts_edge_list, columns=['source', 'target', 'weight'])\n",
    "    reposts_df['source'] = reposts_df['source'].astype(str)\n",
    "    reposts_df['target'] = reposts_df['target'].astype(str)\n",
    "    reposts_dict[dataset] = reposts_df\n",
    "    edges = [tuple(x) for x in reposts_df[['source', 'target', 'weight']].values]\n",
    "    g = ig.Graph.TupleList(edges, edge_attrs=['weight'], directed=True)    \n",
    "    y = get_user_labels(dataset)\n",
    "    g.vs['label'] = [y.loc[e['name']] if e['name'] in y.index else -1 for e in g.vs]\n",
    "    # g.vs.select(_degree=0).delete()\n",
    "    g.simplify(multiple=True, loops=True, combine_edges='sum')\n",
    "    g['name'] = dataset\n",
    "    graphs_dict[dataset] = g\n",
    "    filtered_edges = g.es.select(weight_ge=min_weight)\n",
    "    return g.subgraph_edges(filtered_edges)\n",
    "\n",
    "def get_largest_weak_cc(g):\n",
    "    # Assuming 'g' is your igraph Graph\n",
    "    components = g.components(mode='WEAK')  # Find weakly connected components\n",
    "    return components.giant()\n",
    "\n",
    "def get_graph_with_docs(dataset):\n",
    "    if dataset in graphs_with_docs_dict:\n",
    "        return graphs_with_docs_dict[dataset]\n",
    "    G = graphs_dict.get(dataset, get_reposts_graph(dataset))\n",
    "    doc_vectors = doc_vectors_dict.get(dataset, get_doc_vectors(dataset))\n",
    "    H = G.subgraph(doc_vectors).copy()\n",
    "    graphs_with_docs_dict[dataset] = H\n",
    "    return H\n",
    "\n",
    "def get_label_count_and_percent(dataset):\n",
    "    G = get_graph_with_docs(dataset)\n",
    "    y = get_user_labels(dataset)\n",
    "    ngbrs_labels = {n : np.array([y[ngbr] if ngbr in y.index else 2 for ngbr in G.neighbors(n)]) for n in tqdm(G.nodes())}\n",
    "    ngbrs_labels_count = {k: Counter(v) for k,v in ngbrs_labels.items()}\n",
    "    label_count_df = pd.DataFrame.from_dict(ngbrs_labels_count, orient='index').fillna(0).astype(int).sort_index(axis=0).sort_index(axis=1)\n",
    "    return label_count_df\n",
    "\n",
    "def power_law(x, alpha, C):\n",
    "    return C * x**(-alpha)\n",
    "\n",
    "def get_labeled_nodes(dataset: str = '', g: ig.Graph = None):\n",
    "    if dataset == '' and g is not None:\n",
    "        dataset = g['name']\n",
    "    elif g is None and dataset!='':\n",
    "        g = graphs_dict[dataset]\n",
    "    else:\n",
    "        raise ValueError(\"Both @dataset and @g can't be empty\")\n",
    "    y = get_user_labels(dataset)\n",
    "    labeled_nodes = g.vs.select(lambda v:  v['label'] != -1)\n",
    "    return labeled_nodes\n",
    "\n",
    "# Replace 'degrees' with your actual data containing node degrees\n",
    "def calc_power_law_exp(degrees):\n",
    "\n",
    "    # Convert the degrees to numpy arrays\n",
    "    x = np.array(degrees)\n",
    "\n",
    "    # Count the frequency of each degree\n",
    "    degree_counts = np.bincount(x)\n",
    "\n",
    "    # Remove the zero-degree entries\n",
    "    x = np.nonzero(degree_counts)[0]\n",
    "    y = degree_counts[x]\n",
    "\n",
    "    # Perform the curve fitting\n",
    "    popt, _ = curve_fit(power_law, x, y)\n",
    "    alpha, C = popt\n",
    "    \n",
    "    plt.scatter(x, y, label=\"Data\")\n",
    "    plt.plot(x, power_law(x, alpha, C), color='red', label=f\"Power-law fit (alpha={alpha:.2f})\")\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel(\"Degree\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.title(f'{dataset} Power Law')\n",
    "    plt.savefig(f'detection/experiments/{dataset}_power_law.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    return alpha, C\n",
    "\n",
    "def get_ego_subgraph(g, vertices, order_k = 1):\n",
    "    ego_network = g.neighborhood(vertices=vertices, order=order_k, mode='out')\n",
    "    set_of_tuples = set(tuple(inner_list) for inner_list in ego_network)\n",
    "    flattened_set = list({element for tupl in set_of_tuples for element in tupl})\n",
    "    ego_subgraph = g.subgraph(flattened_set)\n",
    "    return ego_subgraph"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
